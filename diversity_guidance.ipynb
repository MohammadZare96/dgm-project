{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test-Time Diversity Steering via Early-Step Clean Image Estimation\n",
    "\n",
    "This notebook implements and evaluates **Clean-Manifold Diversity Guidance** — a training-free method to improve diversity in text-to-image diffusion models at inference time.\n",
    "\n",
    "**Three methods compared:**\n",
    "| Method | Guidance domain | Description |\n",
    "|---|---|---|\n",
    "| `baseline` | — | Standard CFG sampling |\n",
    "| `naive` | Noisy latent $x_t$ | Diversity gradient on noise-corrupted signal |\n",
    "| `clean_estimate` | Clean estimate $\\hat{x}_0$ | **Ours** — diversity gradient on Tweedie estimate |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch torchvision diffusers transformers accelerate lpips matplotlib tqdm numpy Pillow image-reward scipy\n",
    "!pip install -q git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: Tesla T4\n",
      "VRAM: 15.6 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict, Any\n",
    "import json, os, gc\n",
    "\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# Helper functions\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def predict_x0(x_t, model_output, alpha_prod_t, prediction_type):\n",
    "    \"\"\"Compute x̂₀ from model output via Tweedie's formula.\"\"\"\n",
    "    if prediction_type == \"epsilon\":\n",
    "        x0 = (x_t - (1 - alpha_prod_t) ** 0.5 * model_output) / alpha_prod_t ** 0.5\n",
    "    elif prediction_type == \"v_prediction\":\n",
    "        x0 = alpha_prod_t ** 0.5 * x_t - (1 - alpha_prod_t) ** 0.5 * model_output\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown prediction type: {prediction_type}\")\n",
    "    return x0.clamp(-20, 20)\n",
    "\n",
    "\n",
    "def x0_to_model_output(x_t, x0, alpha_prod_t, prediction_type):\n",
    "    \"\"\"Convert (possibly modified) x̂₀ back to model-output space.\"\"\"\n",
    "    if prediction_type == \"epsilon\":\n",
    "        return (x_t - alpha_prod_t ** 0.5 * x0) / (1 - alpha_prod_t) ** 0.5\n",
    "    elif prediction_type == \"v_prediction\":\n",
    "        return (alpha_prod_t ** 0.5 * x_t - x0) / (1 - alpha_prod_t) ** 0.5\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown prediction type: {prediction_type}\")\n",
    "\n",
    "\n",
    "def compute_diversity_gradient(tensor, loss_type=\"cosine\"):\n",
    "    \"\"\"\n",
    "    Gradient of pairwise similarity loss w.r.t. tensor.\n",
    "\n",
    "    loss_type:\n",
    "      \"cosine\"    – cosine sim on full flattened latent (texture-level)\n",
    "      \"structural\" – cosine sim on avg-pooled latent    (layout-level, recommended)\n",
    "      \"mse\"       – negative pairwise MSE\n",
    "\n",
    "    Returns  (grad [B,C,H,W],  loss_value float)\n",
    "    Gradient is NOT unit-normalised — its magnitude reflects actual similarity.\n",
    "    \"\"\"\n",
    "    B = tensor.shape[0]\n",
    "    if B < 2:\n",
    "        return torch.zeros_like(tensor), 0.0\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        x = tensor.detach().float().clone()\n",
    "        x.requires_grad_(True)\n",
    "\n",
    "        if loss_type == \"structural\":\n",
    "            # avg-pool to 8×8 → captures global composition, ignores texture\n",
    "            pooled = F.adaptive_avg_pool2d(x, (8, 8))\n",
    "            flat = pooled.reshape(B, -1)\n",
    "        elif loss_type in (\"cosine\", \"mse\"):\n",
    "            flat = x.reshape(B, -1)\n",
    "        else:\n",
    "            raise ValueError(loss_type)\n",
    "\n",
    "        if loss_type in (\"cosine\", \"structural\"):\n",
    "            normed = F.normalize(flat, dim=-1)\n",
    "            sim = torch.mm(normed, normed.t())\n",
    "            mask = ~torch.eye(B, dtype=torch.bool, device=flat.device)\n",
    "            loss = sim[mask].mean()\n",
    "        elif loss_type == \"mse\":\n",
    "            diffs = flat.unsqueeze(0) - flat.unsqueeze(1)\n",
    "            sq_dist = (diffs ** 2).sum(-1)\n",
    "            mask = ~torch.eye(B, dtype=torch.bool, device=flat.device)\n",
    "            loss = -sq_dist[mask].mean()\n",
    "\n",
    "        grad = torch.autograd.grad(loss, x)[0]\n",
    "\n",
    "    # normalise so that ||grad||_batch = 1  (scale is controlled by λ only)\n",
    "    batch_norm = grad.norm() + 1e-8\n",
    "    grad = grad / batch_norm\n",
    "    return grad.to(tensor.dtype), loss.item()\n",
    "\n",
    "\n",
    "def diversity_weight(step_idx, num_guidance_steps, schedule=\"constant\"):\n",
    "    \"\"\"Scalar weight in [0,1] for the diversity term at step_idx.\"\"\"\n",
    "    if num_guidance_steps == 0:\n",
    "        return 0.0\n",
    "    ratio = step_idx / max(num_guidance_steps - 1, 1)\n",
    "    if schedule == \"constant\":\n",
    "        return 1.0\n",
    "    elif schedule == \"linear_decay\":\n",
    "        return 1.0 - ratio\n",
    "    elif schedule == \"cosine_decay\":\n",
    "        return 0.5 * (1 + np.cos(np.pi * ratio))\n",
    "    raise ValueError(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# Diversity-Guided Pipeline\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class DiversityGuidedPipeline:\n",
    "    def __init__(self, model_id, device=\"cuda\", dtype=torch.float16):\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            model_id, torch_dtype=dtype, safety_checker=None,\n",
    "        )\n",
    "        self.pipe.scheduler = DDIMScheduler.from_config(\n",
    "            self.pipe.scheduler.config,\n",
    "            clip_sample=False,\n",
    "        )\n",
    "        self.pipe = self.pipe.to(device)\n",
    "\n",
    "        # ── memory optimisations (essential for 16 GB GPUs) ──\n",
    "        self.pipe.enable_vae_slicing()          # decode one image at a time\n",
    "        self.pipe.enable_attention_slicing(1)    # slice attention to save VRAM\n",
    "        try:\n",
    "            self.pipe.enable_xformers_memory_efficient_attention()\n",
    "            print(\"xformers enabled\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        self.prediction_type = self.pipe.scheduler.config.prediction_type\n",
    "        print(f\"Loaded {model_id}  prediction_type={self.prediction_type}\")\n",
    "\n",
    "    def _encode_prompt(self, prompt, negative_prompt, batch_size):\n",
    "        tok, enc = self.pipe.tokenizer, self.pipe.text_encoder\n",
    "        ids = tok(prompt, padding=\"max_length\",\n",
    "                  max_length=tok.model_max_length,\n",
    "                  truncation=True, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        p_emb = enc(ids)[0].repeat(batch_size, 1, 1)\n",
    "        neg = negative_prompt or \"\"\n",
    "        nids = tok(neg, padding=\"max_length\",\n",
    "                   max_length=tok.model_max_length,\n",
    "                   truncation=True, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        n_emb = enc(nids)[0].repeat(batch_size, 1, 1)\n",
    "        return p_emb, n_emb\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        batch_size: int = 4,\n",
    "        num_inference_steps: int = 50,\n",
    "        guidance_scale: float = 9.0,\n",
    "        diversity_scale: float = 30.0,\n",
    "        early_stop_ratio: float = 0.2,\n",
    "        method: str = \"clean_estimate\",\n",
    "        loss_type: str = \"structural\",\n",
    "        weight_schedule: str = \"constant\",\n",
    "        seeds: list = None,\n",
    "        height: int = 512,\n",
    "        width: int = 512,\n",
    "        negative_prompt: str = \"\",\n",
    "    ) -> Dict[str, Any]:\n",
    "\n",
    "        if seeds is None:\n",
    "            seeds = list(range(42, 42 + batch_size))\n",
    "\n",
    "        p_emb, n_emb = self._encode_prompt(prompt, negative_prompt, batch_size)\n",
    "\n",
    "        latent_ch = self.pipe.unet.config.in_channels\n",
    "        lh, lw = height // 8, width // 8\n",
    "        gens = [torch.Generator(device=self.device).manual_seed(s)\n",
    "                for s in seeds]\n",
    "        latents = torch.cat([\n",
    "            torch.randn(1, latent_ch, lh, lw, generator=g,\n",
    "                        device=self.device, dtype=self.dtype)\n",
    "            for g in gens\n",
    "        ])\n",
    "\n",
    "        self.pipe.scheduler.set_timesteps(num_inference_steps, device=self.device)\n",
    "        timesteps = self.pipe.scheduler.timesteps\n",
    "        latents = latents * self.pipe.scheduler.init_noise_sigma\n",
    "\n",
    "        n_guide = int(len(timesteps) * early_stop_ratio) if method != \"baseline\" else 0\n",
    "        div_losses = []\n",
    "\n",
    "        for si, t in enumerate(tqdm(timesteps, leave=False, desc=method)):\n",
    "            do_guide = si < n_guide\n",
    "\n",
    "            lat_in = torch.cat([latents] * 2)\n",
    "            lat_in = self.pipe.scheduler.scale_model_input(lat_in, t)\n",
    "            emb_in = torch.cat([n_emb, p_emb])\n",
    "\n",
    "            noise_pred = self.pipe.unet(lat_in, t,\n",
    "                                        encoder_hidden_states=emb_in).sample\n",
    "            n_unc, n_cond = noise_pred.chunk(2)\n",
    "            n_cfg = n_unc + guidance_scale * (n_cond - n_unc)\n",
    "\n",
    "            if do_guide:\n",
    "                w = diversity_weight(si, n_guide, weight_schedule)\n",
    "                apt = self.pipe.scheduler.alphas_cumprod[t.long()]\n",
    "\n",
    "                if method == \"naive\":\n",
    "                    grad, lv = compute_diversity_gradient(latents, loss_type)\n",
    "                    latents = self.pipe.scheduler.step(n_cfg, t, latents).prev_sample\n",
    "                    latents = latents - (diversity_scale * w) * grad\n",
    "                    div_losses.append(lv)\n",
    "                    continue\n",
    "\n",
    "                elif method == \"clean_estimate\":\n",
    "                    x0h = predict_x0(latents, n_cfg, apt, self.prediction_type)\n",
    "                    grad, lv = compute_diversity_gradient(x0h, loss_type)\n",
    "                    latents = self.pipe.scheduler.step(n_cfg, t, latents).prev_sample\n",
    "                    latents = latents - (diversity_scale * w) * grad\n",
    "                    div_losses.append(lv)\n",
    "                    continue\n",
    "\n",
    "            latents = self.pipe.scheduler.step(n_cfg, t, latents).prev_sample\n",
    "\n",
    "        # decode with slicing (one image at a time → constant VRAM)\n",
    "        imgs_t = self.pipe.vae.decode(\n",
    "            latents / self.pipe.vae.config.scaling_factor\n",
    "        ).sample\n",
    "        imgs_t = (imgs_t / 2 + 0.5).clamp(0, 1)\n",
    "        imgs_pil = [\n",
    "            Image.fromarray(\n",
    "                (im.permute(1, 2, 0).cpu().float().numpy() * 255).astype(np.uint8)\n",
    "            ) for im in imgs_t\n",
    "        ]\n",
    "\n",
    "        # free intermediate VRAM\n",
    "        del latents, noise_pred, lat_in, n_cfg\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return {\"images\": imgs_pil, \"images_tensor\": imgs_t.cpu(),\n",
    "                \"diversity_losses\": div_losses}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched transformers compatibility for ImageReward\n"
     ]
    }
   ],
   "source": [
    "# ── Patch transformers compatibility for ImageReward ──────────\n",
    "# ImageReward's BLIP imports functions that moved in transformers >= 4.45\n",
    "import transformers.modeling_utils as _mu\n",
    "try:\n",
    "    from transformers.pytorch_utils import (\n",
    "        apply_chunking_to_forward,\n",
    "        find_pruneable_heads_and_indices,\n",
    "        prune_linear_layer,\n",
    "    )\n",
    "    for _name, _fn in [\n",
    "        (\"apply_chunking_to_forward\", apply_chunking_to_forward),\n",
    "        (\"find_pruneable_heads_and_indices\", find_pruneable_heads_and_indices),\n",
    "        (\"prune_linear_layer\", prune_linear_layer),\n",
    "    ]:\n",
    "        if not hasattr(_mu, _name):\n",
    "            setattr(_mu, _name, _fn)\n",
    "    print(\"Patched transformers compatibility for ImageReward\")\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "class DiversityMetrics:\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.device = device\n",
    "        self._lpips = None\n",
    "        self._clip_model = None\n",
    "        self._clip_proc = None\n",
    "        self._inception = None\n",
    "        self._reward = None\n",
    "\n",
    "    # ── Lazy-loaded models ──────────────────────────────────\n",
    "\n",
    "    @property\n",
    "    def lpips_fn(self):\n",
    "        if self._lpips is None:\n",
    "            import lpips\n",
    "            self._lpips = lpips.LPIPS(net=\"alex\").to(self.device).eval()\n",
    "        return self._lpips\n",
    "\n",
    "    def _load_clip(self):\n",
    "        from transformers import CLIPModel, CLIPProcessor\n",
    "        name = \"openai/clip-vit-base-patch32\"\n",
    "        self._clip_model = CLIPModel.from_pretrained(name).to(self.device).eval()\n",
    "        self._clip_proc = CLIPProcessor.from_pretrained(name)\n",
    "\n",
    "    @property\n",
    "    def clip_model(self):\n",
    "        if self._clip_model is None: self._load_clip()\n",
    "        return self._clip_model\n",
    "\n",
    "    @property\n",
    "    def clip_proc(self):\n",
    "        if self._clip_proc is None: self._load_clip()\n",
    "        return self._clip_proc\n",
    "\n",
    "    @property\n",
    "    def inception_model(self):\n",
    "        if self._inception is None:\n",
    "            from torchvision.models import inception_v3\n",
    "            self._inception = inception_v3(pretrained=True).to(self.device).eval()\n",
    "            print(\"Inception-v3 loaded for IS/FID\")\n",
    "        return self._inception\n",
    "\n",
    "    @property\n",
    "    def reward_model(self):\n",
    "        if self._reward is None:\n",
    "            import ImageReward as RM\n",
    "            self._reward = RM.load(\"ImageReward-v1.0\", device=self.device)\n",
    "            print(\"ImageReward-v1.0 loaded\")\n",
    "        return self._reward\n",
    "\n",
    "    # ── Feature extraction helpers ──────────────────────────\n",
    "\n",
    "    def _get_image_features(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        inputs = self.clip_proc(images=images, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].to(self.device)\n",
    "        feats = self.clip_model.get_image_features(pixel_values=pixel_values)\n",
    "        if not isinstance(feats, torch.Tensor):\n",
    "            feats = feats.pooler_output if hasattr(feats, \"pooler_output\") else feats[0]\n",
    "        return feats\n",
    "\n",
    "    def _get_text_features(self, text: str) -> torch.Tensor:\n",
    "        inputs = self.clip_proc(text=[text], return_tensors=\"pt\", padding=True)\n",
    "        input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
    "        feats = self.clip_model.get_text_features(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        )\n",
    "        if not isinstance(feats, torch.Tensor):\n",
    "            feats = feats.pooler_output if hasattr(feats, \"pooler_output\") else feats[0]\n",
    "        return feats\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_inception_outputs(self, images: List[Image.Image], batch_size=32):\n",
    "        \"\"\"Extract Inception-v3 pool features (2048-d) and class probs (1000-d).\"\"\"\n",
    "        from torchvision import transforms as TF\n",
    "        transform = TF.Compose([\n",
    "            TF.Resize((299, 299), interpolation=TF.InterpolationMode.BILINEAR),\n",
    "            TF.ToTensor(),\n",
    "        ])\n",
    "        model = self.inception_model\n",
    "        feat_buf = []\n",
    "        hook = model.avgpool.register_forward_hook(\n",
    "            lambda m, inp, out: feat_buf.append(out.flatten(1))\n",
    "        )\n",
    "        all_feats, all_probs = [], []\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch = images[i:i+batch_size]\n",
    "            tensors = torch.stack(\n",
    "                [transform(img.convert(\"RGB\")) for img in batch]\n",
    "            ).to(self.device)\n",
    "            feat_buf.clear()\n",
    "            logits = model(tensors)\n",
    "            if isinstance(logits, tuple):\n",
    "                logits = logits[0]\n",
    "            all_probs.append(F.softmax(logits, dim=1).cpu())\n",
    "            all_feats.append(feat_buf[0].cpu())\n",
    "        hook.remove()\n",
    "        return torch.cat(all_feats), torch.cat(all_probs)\n",
    "\n",
    "    # ── Per-batch diversity metrics ─────────────────────────\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def pairwise_lpips(self, images: List[Image.Image]) -> float:\n",
    "        tensors = []\n",
    "        for img in images:\n",
    "            arr = np.array(img.resize((256, 256))).astype(np.float32) / 255.0\n",
    "            tensors.append(torch.from_numpy(arr).permute(2, 0, 1) * 2 - 1)\n",
    "        tensors = torch.stack(tensors).to(self.device)\n",
    "        vals = [self.lpips_fn(tensors[i:i+1], tensors[j:j+1]).item()\n",
    "                for i, j in combinations(range(len(images)), 2)]\n",
    "        return float(np.mean(vals)) if vals else 0.0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def clip_diversity(self, images: List[Image.Image]) -> float:\n",
    "        feats = F.normalize(self._get_image_features(images), dim=-1)\n",
    "        sim = torch.mm(feats, feats.t())\n",
    "        B = len(images)\n",
    "        mask = ~torch.eye(B, dtype=torch.bool, device=self.device)\n",
    "        return 1.0 - sim[mask].mean().item()\n",
    "\n",
    "    # ── Per-batch alignment metrics ─────────────────────────\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def clip_score(self, images: List[Image.Image], prompt: str) -> float:\n",
    "        img_feats  = F.normalize(self._get_image_features(images), dim=-1)\n",
    "        text_feats = F.normalize(self._get_text_features(prompt),  dim=-1)\n",
    "        sims = (img_feats @ text_feats.T).squeeze(-1)\n",
    "        return sims.mean().item()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def image_reward_score(self, images: List[Image.Image], prompt: str) -> float:\n",
    "        \"\"\"Mean ImageReward score across the batch.\"\"\"\n",
    "        scores = []\n",
    "        for img in images:\n",
    "            s = self.reward_model.score(prompt, img)\n",
    "            scores.append(float(s))\n",
    "        return float(np.mean(scores))\n",
    "\n",
    "    # ── Aggregate quality metrics ───────────────────────────\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inception_score(self, images: List[Image.Image], splits: int = 5):\n",
    "        \"\"\"Inception Score over a set of images.  Returns (mean, std).\"\"\"\n",
    "        _, probs = self._get_inception_outputs(images)\n",
    "        N = len(probs)\n",
    "        splits = min(splits, N)\n",
    "        scores = []\n",
    "        for k in range(splits):\n",
    "            part = probs[k * N // splits : (k + 1) * N // splits]\n",
    "            if len(part) < 2:\n",
    "                continue\n",
    "            py = part.mean(0, keepdim=True)\n",
    "            kl = (part * (part.log() - py.log())).sum(1).mean()\n",
    "            scores.append(kl.exp().item())\n",
    "        return (float(np.mean(scores)), float(np.std(scores))) if scores else (0.0, 0.0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_fid(self, real_images: List[Image.Image],\n",
    "                    fake_images: List[Image.Image]) -> float:\n",
    "        \"\"\"FID between two image sets.\"\"\"\n",
    "        real_f, _ = self._get_inception_outputs(real_images)\n",
    "        fake_f, _ = self._get_inception_outputs(fake_images)\n",
    "        return self._fid_from_features(real_f, fake_f)\n",
    "\n",
    "    @staticmethod\n",
    "    def _fid_from_features(feats1: torch.Tensor, feats2: torch.Tensor) -> float:\n",
    "        \"\"\"Compute FID from pre-extracted Inception features.\"\"\"\n",
    "        from scipy import linalg\n",
    "        f1 = feats1.double().numpy()\n",
    "        f2 = feats2.double().numpy()\n",
    "        mu1, mu2 = f1.mean(0), f2.mean(0)\n",
    "        s1 = np.cov(f1, rowvar=False) + np.eye(f1.shape[1]) * 1e-6\n",
    "        s2 = np.cov(f2, rowvar=False) + np.eye(f2.shape[1]) * 1e-6\n",
    "        diff = mu1 - mu2\n",
    "        covmean, _ = linalg.sqrtm(s1 @ s2, disp=False)\n",
    "        if np.iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "        return float(diff @ diff + np.trace(s1 + s2 - 2 * covmean))\n",
    "\n",
    "    # ── Combined evaluation ─────────────────────────────────\n",
    "\n",
    "    def evaluate_batch(self, images, prompt):\n",
    "        return {\n",
    "            \"lpips_diversity\": self.pairwise_lpips(images),\n",
    "            \"clip_score\":      self.clip_score(images, prompt),\n",
    "            \"clip_diversity\":  self.clip_diversity(images),\n",
    "            \"image_reward\":    self.image_reward_score(images, prompt),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmark Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prompts: 40  (diversity: 20, geneval: 20)\n"
     ]
    }
   ],
   "source": [
    "DIVERSITY_PROMPTS = [\n",
    "    \"a cat sitting\",\n",
    "    \"a dog playing in a field\",\n",
    "    \"a red sports car\",\n",
    "    \"a bouquet of flowers in a vase\",\n",
    "    \"a bird flying over the ocean\",\n",
    "    \"a beautiful mountain landscape\",\n",
    "    \"a city street at night with neon lights\",\n",
    "    \"a cozy living room with a fireplace\",\n",
    "    \"a tropical beach at sunset\",\n",
    "    \"a forest path in autumn\",\n",
    "    \"portrait of a woman with curly hair\",\n",
    "    \"a man walking in the rain with an umbrella\",\n",
    "    \"a child playing in a park\",\n",
    "    \"an oil painting of a castle on a hill\",\n",
    "    \"a photograph of a steaming coffee cup\",\n",
    "    \"a watercolor painting of a sailboat on a lake\",\n",
    "    \"a futuristic city skyline\",\n",
    "    \"a magical forest with glowing mushrooms\",\n",
    "    \"a steampunk robot in a workshop\",\n",
    "    \"an astronaut floating in outer space\",\n",
    "]\n",
    "\n",
    "GENEVAL_PROMPTS = [\n",
    "    \"a blue cube on a white surface\",\n",
    "    \"a red sphere on a gray background\",\n",
    "    \"a green pyramid on a wooden table\",\n",
    "    \"a yellow cylinder on a dark surface\",\n",
    "    \"a red cube and a blue sphere\",\n",
    "    \"a cat and a dog sitting together\",\n",
    "    \"a car next to a tree\",\n",
    "    \"an apple and a banana on a plate\",\n",
    "    \"a cat sitting on a wooden table\",\n",
    "    \"a ball under a red chair\",\n",
    "    \"a bird perched above a birdhouse\",\n",
    "    \"a bicycle in front of a brick building\",\n",
    "    \"two cats sleeping on a couch\",\n",
    "    \"three red apples on a wooden table\",\n",
    "    \"four colorful birds sitting on a wire\",\n",
    "    \"a red car and a blue bicycle on a street\",\n",
    "    \"a green frog and an orange butterfly in a garden\",\n",
    "    \"a purple hat on a brown wooden table\",\n",
    "    \"a white cat and a black dog on grass\",\n",
    "    \"a pink flower in a blue vase\",\n",
    "    \"a beautiful cinematic shark\",\n",
    "    \"a majestic knight standing in a mystrical forest, cinematic lighting, highly detailed\",\n",
    "    \"a beautiful cartoonlike village full of trees and houses.\"\n",
    "]\n",
    "\n",
    "ALL_PROMPTS = DIVERSITY_PROMPTS + GENEVAL_PROMPTS\n",
    "print(f\"Total prompts: {len(ALL_PROMPTS)}  \"\n",
    "      f\"(diversity: {len(DIVERSITY_PROMPTS)}, geneval: {len(GENEVAL_PROMPTS)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration\n",
    "\n",
    "**Four runs:**\n",
    "| # | Model | Method | CFG | Purpose |\n",
    "|---|---|---|---|---|\n",
    "| 1 | `runwayml/stable-diffusion-v1-5` | baseline | 7 | Reference — base model with standard CFG |\n",
    "| 2 | `Lykon/dreamshaper-8` | baseline | 7 | Aligned fine-tune — shows reduced diversity |\n",
    "| 3 | `Lykon/dreamshaper-8` | naive | 7 | Diversity guidance on noisy latent |\n",
    "| 4 | `Lykon/dreamshaper-8` | clean_estimate | 7 | **Ours** — diversity guidance on clean estimate |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will run 4 configurations × 1 prompts = 4 generations\n"
     ]
    }
   ],
   "source": [
    "# ─── Experiment configuration ────────────────────────────────────────\n",
    "\n",
    "# Each run: (model_id, method, guidance_scale, diversity_scale)\n",
    "RUNS = [\n",
    "    (\"runwayml/stable-diffusion-v1-5\", \"baseline\",       3.5,  0.0),\n",
    "    (\"Lykon/dreamshaper-8\",            \"baseline\",       7.5,  0.0),\n",
    "    (\"Lykon/dreamshaper-8\",            \"naive\",          7.5, 10.0),\n",
    "    (\"Lykon/dreamshaper-8\",            \"clean_estimate\", 7.5, 10.0),\n",
    "]\n",
    "\n",
    "BATCH_SIZE       = 4\n",
    "NUM_STEPS        = 50\n",
    "HEIGHT, WIDTH    = 512, 512\n",
    "SEEDS            = [42, 100, 1234, 9999]   # one per image in the batch\n",
    "NEGATIVE_PROMPT  = \"\"\n",
    "\n",
    "# ─── Diversity guidance settings ─────────────────────────────────────\n",
    "EARLY_STOP_RATIO  = 0.3\n",
    "LOSS_TYPE         = \"structural\"\n",
    "WEIGHT_SCHEDULE   = \"constant\"\n",
    "\n",
    "# ─── Prompt selection ────────────────────────────────────────────────\n",
    "PROMPTS = [\"a majestic knight standing in a mystical forest, cinematic lighting, highly detailed.\"]\n",
    "NUM_PROMPTS = 10\n",
    "\n",
    "if NUM_PROMPTS is not None:\n",
    "    PROMPTS = PROMPTS[:NUM_PROMPTS]\n",
    "\n",
    "assert len(SEEDS) == BATCH_SIZE, f\"Need {BATCH_SIZE} seeds, got {len(SEEDS)}\"\n",
    "print(f\"Will run {len(RUNS)} configurations × {len(PROMPTS)} prompts = \"\n",
    "      f\"{len(RUNS) * len(PROMPTS)} generations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Model & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics module ready (models loaded lazily)\n"
     ]
    }
   ],
   "source": [
    "metrics = DiversityMetrics(device=device)\n",
    "print(\"Metrics module ready (models loaded lazily)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  MODEL: runwayml/stable-diffusion-v1-5\n",
      "══════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01f9865c34543f59246e9cd193e3483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded runwayml/stable-diffusion-v1-5  prediction_type=epsilon\n",
      "\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "  stable-diffusion-v1-5/baseline/CFG=3.5/λ=0.0   early_stop=0.4\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d285bcdbba4e898e356526788c1255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "baseline:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [ 1/1] LPIPS=0.6498  CLIP=0.3392  CLIPdiv=0.1518  IR=0.3464  │ a majestic knight standing in a mystical\n",
      "\n",
      "  Freed stable-diffusion-v1-5 from GPU\n",
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  MODEL: Lykon/dreamshaper-8\n",
      "══════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2206c0aea5724f2a8643edda0a3ca6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Lykon/dreamshaper-8  prediction_type=epsilon\n",
      "\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "  dreamshaper-8/baseline/CFG=7.5/λ=0.0   early_stop=0.4\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec15625aade24c6ab8615e2fadc6ebcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "baseline:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [ 1/1] LPIPS=0.4571  CLIP=0.3565  CLIPdiv=0.0708  IR=0.9113  │ a majestic knight standing in a mystical\n",
      "\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "  dreamshaper-8/naive/CFG=7.5/λ=10.0   early_stop=0.4\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0188ff7f3846a9a0fea86ef331e26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "naive:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [ 1/1] LPIPS=0.5847  CLIP=0.3515  CLIPdiv=0.0729  IR=0.5347  │ a majestic knight standing in a mystical\n",
      "\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "  dreamshaper-8/clean_estimate/CFG=7.5/λ=10.0   early_stop=0.4\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53abb7c597c54291b6c3c0bf109552ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "clean_estimate:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [ 1/1] LPIPS=0.7822  CLIP=0.3589  CLIPdiv=0.1446  IR=0.6476  │ a majestic knight standing in a mystical\n",
      "\n",
      "  Freed dreamshaper-8 from GPU\n",
      "\n",
      "Results saved to outputs/results.json\n"
     ]
    }
   ],
   "source": [
    "output_dir = Path(\"outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "all_results  = {}   # key → list[dict]\n",
    "image_store  = {}   # key → list[list[PIL]]\n",
    "\n",
    "def model_short(model_id):\n",
    "    return model_id.split(\"/\")[-1]\n",
    "\n",
    "def run_key(model_id, method, cfg, div_scale):\n",
    "    return f\"{model_short(model_id)}/{method}/CFG={cfg}/λ={div_scale}\"\n",
    "\n",
    "# Group runs by model, preserving the order defined in RUNS\n",
    "from collections import OrderedDict\n",
    "runs_by_model = OrderedDict()\n",
    "for r in RUNS:\n",
    "    runs_by_model.setdefault(r[0], []).append(r)\n",
    "\n",
    "for model_id, group in runs_by_model.items():\n",
    "    mname = model_short(model_id)\n",
    "    print(f\"\\n{'═' * 70}\")\n",
    "    print(f\"  MODEL: {model_id}\")\n",
    "    print(f\"{'═' * 70}\")\n",
    "\n",
    "    pipeline = DiversityGuidedPipeline(model_id, device=device)\n",
    "\n",
    "    for model_id_r, method, cfg, div_scale in group:\n",
    "        key = run_key(model_id_r, method, cfg, div_scale)\n",
    "        print(f\"\\n{'━' * 70}\")\n",
    "        print(f\"  {key}   early_stop={EARLY_STOP_RATIO}\")\n",
    "        print(f\"{'━' * 70}\")\n",
    "\n",
    "        results_list = []\n",
    "        images_list  = []\n",
    "\n",
    "        for pi, prompt in enumerate(PROMPTS):\n",
    "            out = pipeline.generate(\n",
    "                prompt=prompt,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                num_inference_steps=NUM_STEPS,\n",
    "                guidance_scale=cfg,\n",
    "                diversity_scale=div_scale,\n",
    "                early_stop_ratio=EARLY_STOP_RATIO,\n",
    "                method=method,\n",
    "                loss_type=LOSS_TYPE,\n",
    "                weight_schedule=WEIGHT_SCHEDULE,\n",
    "                seeds=SEEDS,\n",
    "                height=HEIGHT, width=WIDTH,\n",
    "                negative_prompt=NEGATIVE_PROMPT,\n",
    "            )\n",
    "\n",
    "            m = metrics.evaluate_batch(out[\"images\"], prompt)\n",
    "            m[\"prompt\"] = prompt\n",
    "            m[\"diversity_losses\"] = out[\"diversity_losses\"]\n",
    "            results_list.append(m)\n",
    "            images_list.append(out[\"images\"])\n",
    "\n",
    "            print(f\"  [{pi+1:2d}/{len(PROMPTS)}] \"\n",
    "                  f\"LPIPS={m['lpips_diversity']:.4f}  \"\n",
    "                  f\"CLIP={m['clip_score']:.4f}  \"\n",
    "                  f\"CLIPdiv={m['clip_diversity']:.4f}  \"\n",
    "                  f\"IR={m['image_reward']:.4f}  │ {prompt[:40]}\")\n",
    "\n",
    "        all_results[key] = results_list\n",
    "        image_store[key] = images_list\n",
    "\n",
    "    del pipeline\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"\\n  Freed {mname} from GPU\")\n",
    "\n",
    "# save raw results\n",
    "with open(output_dir / \"results.json\", \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2, default=str)\n",
    "print(f\"\\nResults saved to {output_dir / 'results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4 images -> outputs/dreamshaper-8_baseline_λ0.0\n",
      "Saved 4 images -> outputs/dreamshaper-8_naive_λ10\n",
      "Saved 4 images -> outputs/dreamshaper-8_clean_estimate_λ10\n",
      "\n",
      "── Inception Score (per method) ──\n",
      "Inception-v3 loaded for IS/FID\n",
      "  dreamshaper-8/baseline/λ=0.0                        IS = 0.00 +/- 0.00\n",
      "  dreamshaper-8/naive/λ=10                            IS = 0.00 +/- 0.00\n",
      "  dreamshaper-8/clean_estimate/λ=10                   IS = 0.00 +/- 0.00\n",
      "\n",
      "FID skipped -- reference images not found at outputs/val2017\n",
      "  To enable FID, run:\n",
      "  !wget http://images.cocodataset.org/zips/val2017.zip -O outputs/val2017.zip\n",
      "  !unzip -q outputs/val2017.zip -d outputs/ && rm outputs/val2017.zip\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# 7.5  Aggregate Quality Metrics — Inception Score & FID\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "# Save generated images to per-method folders\n",
    "for key, images_list in image_store.items():\n",
    "    method_dir = output_dir / key.replace(\"/\", \"_\").replace(\"=\", \"\")\n",
    "    method_dir.mkdir(parents=True, exist_ok=True)\n",
    "    idx = 0\n",
    "    for batch in images_list:\n",
    "        for img in batch:\n",
    "            img.save(method_dir / f\"{idx:04d}.png\")\n",
    "            idx += 1\n",
    "    print(f\"Saved {idx} images -> {method_dir}\")\n",
    "\n",
    "# ── Inception Score (no reference needed) ──\n",
    "print(\"\\n── Inception Score (per method) ──\")\n",
    "aggregate_metrics = {}\n",
    "for key, images_list in image_store.items():\n",
    "    all_imgs = [img for batch in images_list for img in batch]\n",
    "    is_mean, is_std = metrics.inception_score(all_imgs)\n",
    "    aggregate_metrics[key] = {\"is_mean\": is_mean, \"is_std\": is_std}\n",
    "    print(f\"  {key:<50}  IS = {is_mean:.2f} +/- {is_std:.2f}\")\n",
    "\n",
    "# ── FID (requires reference real images) ──\n",
    "FID_REF_DIR = Path(\"outputs/val2017\")\n",
    "\n",
    "if FID_REF_DIR.exists():\n",
    "    ref_paths = sorted(FID_REF_DIR.glob(\"*.jpg\"))[:2048]\n",
    "    ref_images = [Image.open(p).convert(\"RGB\")\n",
    "                  for p in tqdm(ref_paths, desc=\"Loading FID reference\")]\n",
    "    print(f\"Loaded {len(ref_images)} reference images\")\n",
    "\n",
    "    ref_feats, _ = metrics._get_inception_outputs(ref_images)\n",
    "\n",
    "    print(\"\\n── FID (per method, vs COCO val2017) ──\")\n",
    "    for key, images_list in image_store.items():\n",
    "        all_imgs = [img for batch in images_list for img in batch]\n",
    "        fake_feats, _ = metrics._get_inception_outputs(all_imgs)\n",
    "        fid_val = metrics._fid_from_features(ref_feats, fake_feats)\n",
    "        aggregate_metrics[key][\"fid\"] = fid_val\n",
    "        print(f\"  {key:<50}  FID = {fid_val:.2f}\")\n",
    "\n",
    "    del ref_images, ref_feats\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(f\"\\nFID skipped -- reference images not found at {FID_REF_DIR}\")\n",
    "    print(\"  To enable FID, run:\")\n",
    "    print(\"  !wget http://images.cocodataset.org/zips/val2017.zip -O outputs/val2017.zip\")\n",
    "    print(\"  !unzip -q outputs/val2017.zip -d outputs/ && rm outputs/val2017.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_fid = any(\"fid\" in aggregate_metrics.get(k, {}) for k in all_results)\n",
    "\n",
    "kw = 55\n",
    "cols = f\"{'Run':<{kw}} {'LPIPS':>7} {'CLIP':>7} {'CLIPdiv':>8} {'ImgRwd':>7} {'IS':>10}\"\n",
    "if has_fid:\n",
    "    cols += f\"  {'FID':>8}\"\n",
    "sep = \"=\" * len(cols)\n",
    "\n",
    "print(sep)\n",
    "print(cols)\n",
    "print(f\"{'':>{kw}} {'div':>7} {'align':>7} {'div':>8} {'align':>7} {'qual':>10}\", end=\"\")\n",
    "if has_fid:\n",
    "    print(f\"  {'qual':>8}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * len(cols))\n",
    "\n",
    "for key, results in all_results.items():\n",
    "    lp = np.mean([r[\"lpips_diversity\"] for r in results])\n",
    "    cs = np.mean([r[\"clip_score\"]      for r in results])\n",
    "    cd = np.mean([r[\"clip_diversity\"]   for r in results])\n",
    "    ir = np.mean([r[\"image_reward\"]    for r in results])\n",
    "\n",
    "    agg = aggregate_metrics.get(key, {})\n",
    "    is_str = f\"{agg['is_mean']:.2f}+/-{agg.get('is_std',0):.2f}\" if \"is_mean\" in agg else \"--\"\n",
    "\n",
    "    row = f\"{key:<{kw}} {lp:>7.4f} {cs:>7.4f} {cd:>8.4f} {ir:>7.4f} {is_str:>10}\"\n",
    "    if has_fid:\n",
    "        fid_str = f\"{agg['fid']:.1f}\" if \"fid\" in agg else \"--\"\n",
    "        row += f\"  {fid_str:>8}\"\n",
    "    print(row)\n",
    "\n",
    "print(sep)\n",
    "print()\n",
    "print(\"Legend:  LPIPS/CLIPdiv = diversity (higher is better)\")\n",
    "print(\"        CLIP/ImgRwd   = alignment (higher is better)\")\n",
    "print(\"        IS            = quality   (higher is better)\")\n",
    "if has_fid:\n",
    "    print(\"        FID           = quality   (lower  is better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Pareto Curve — Diversity vs. Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKERS = {\"baseline\": \"o\", \"naive\": \"s\", \"clean_estimate\": \"^\"}\n",
    "COLORS  = {\"baseline\": \"#d62728\", \"naive\": \"#1f77b4\", \"clean_estimate\": \"#2ca02c\"}\n",
    "# Distinguish models by fill style\n",
    "MODEL_FILL = {\"stable-diffusion-v1-5\": \"full\", \"dreamshaper-8\": \"none\"}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5.5))\n",
    "\n",
    "for ax, div_key, xlabel in [\n",
    "    (axes[0], \"lpips_diversity\",  \"LPIPS Diversity  (higher → more diverse)\"),\n",
    "    (axes[1], \"clip_diversity\",   \"CLIP Feature Diversity  (higher → more diverse)\"),\n",
    "]:\n",
    "    for key, results in all_results.items():\n",
    "        parts = key.split(\"/\")\n",
    "        mname, method = parts[0], parts[1]\n",
    "        divs   = [r[div_key]      for r in results]\n",
    "        clips  = [r[\"clip_score\"] for r in results]\n",
    "        mk = MARKERS.get(method, \"x\")\n",
    "        cl = COLORS.get(method, \"gray\")\n",
    "        fill = MODEL_FILL.get(mname, \"full\")\n",
    "        fc = cl if fill == \"full\" else \"none\"\n",
    "\n",
    "        ax.scatter(divs, clips, marker=mk, facecolors=fc,\n",
    "                   edgecolors=cl, s=18, alpha=0.25)\n",
    "        ax.scatter(np.mean(divs), np.mean(clips), marker=mk,\n",
    "                   facecolors=fc, edgecolors=\"k\", s=120,\n",
    "                   linewidths=0.8, zorder=5, label=key)\n",
    "\n",
    "    ax.set_xlabel(xlabel, fontsize=11)\n",
    "    ax.set_ylabel(\"CLIP Score  (higher → better alignment)\", fontsize=11)\n",
    "    ax.legend(fontsize=6, loc=\"best\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle(\"Diversity vs Quality — SD1.5 (filled) vs DreamShaper-8 (hollow)\",\n",
    "             fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"pareto.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Diversity Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "\n",
    "LINESTYLES = {\"stable-diffusion-v1-5\": \"-\", \"dreamshaper-8\": \"--\"}\n",
    "\n",
    "for key, results in all_results.items():\n",
    "    curves = [r.get(\"diversity_losses\", []) for r in results]\n",
    "    if not any(curves):\n",
    "        continue\n",
    "    max_len = max(len(c) for c in curves)\n",
    "    padded  = [c + [np.nan]*(max_len - len(c)) for c in curves]\n",
    "    avg     = np.nanmean(padded, axis=0)\n",
    "    parts   = key.split(\"/\")\n",
    "    mname, method = parts[0], parts[1]\n",
    "    ax.plot(avg, label=key, color=COLORS.get(method, \"gray\"),\n",
    "            linestyle=LINESTYLES.get(mname, \"-\"))\n",
    "\n",
    "ax.set_xlabel(\"Guided step index\")\n",
    "ax.set_ylabel(\"Pairwise similarity  (lower → more diverse)\")\n",
    "ax.set_title(\"Diversity Loss During Guided Steps\")\n",
    "ax.legend(fontsize=6)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"loss_curves.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Per-Prompt Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(all_results.keys())\n",
    "n_prompts = len(PROMPTS)\n",
    "short = [p[:25]+\"…\" if len(p)>25 else p for p in PROMPTS]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(max(10, n_prompts*0.6), 5))\n",
    "x = np.arange(n_prompts)\n",
    "w = 0.8 / len(keys)\n",
    "\n",
    "# hatch patterns to distinguish models\n",
    "MODEL_HATCH = {\"stable-diffusion-v1-5\": \"\", \"dreamshaper-8\": \"//\"}\n",
    "\n",
    "for mi, key in enumerate(keys):\n",
    "    vals = [r[\"lpips_diversity\"] for r in all_results[key]]\n",
    "    parts = key.split(\"/\")\n",
    "    mname, method = parts[0], parts[1]\n",
    "    ax.bar(x + mi*w, vals, w, label=key,\n",
    "           color=COLORS.get(method, \"gray\"), alpha=0.85,\n",
    "           hatch=MODEL_HATCH.get(mname, \"\"))\n",
    "\n",
    "ax.set_xticks(x + w*(len(keys)-1)/2)\n",
    "ax.set_xticklabels(short, rotation=45, ha=\"right\", fontsize=7)\n",
    "ax.set_ylabel(\"LPIPS Diversity ↑\")\n",
    "ax.set_title(\"Per-Prompt Diversity (hatched = DreamShaper-8)\")\n",
    "ax.legend(fontsize=5, ncol=2)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"per_prompt.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Image Grids — Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_grid(prompt_idx=0, selected_keys=None):\n",
    "    \"\"\"\n",
    "    Show images for one prompt, matching the reference layout:\n",
    "      - Title bar at top with prompt + config\n",
    "      - Left text column with method name + metrics\n",
    "      - Image columns to the right\n",
    "    \"\"\"\n",
    "    if selected_keys is None:\n",
    "        selected_keys = list(image_store.keys())\n",
    "\n",
    "    n_rows = len(selected_keys)\n",
    "    bs = len(image_store[selected_keys[0]][prompt_idx])\n",
    "    prompt_text = all_results[selected_keys[0]][prompt_idx][\"prompt\"]\n",
    "\n",
    "    # Layout: label column (narrow) + bs image columns\n",
    "    label_w = 2.5   # inches for text labels\n",
    "    img_sz  = 3.0   # inches per image cell\n",
    "    fig_w = label_w + img_sz * bs\n",
    "    fig_h = img_sz * n_rows + 0.8  # +title space\n",
    "\n",
    "    fig = plt.figure(figsize=(fig_w, fig_h), dpi=120)\n",
    "    gs = fig.add_gridspec(n_rows, bs + 1,\n",
    "                          width_ratios=[label_w] + [img_sz] * bs,\n",
    "                          wspace=0.05, hspace=0.12)\n",
    "\n",
    "    # ── Title ──\n",
    "    seeds_str = \", \".join(str(s) for s in SEEDS)\n",
    "    fig.suptitle(f'Prompt: \"{prompt_text}\"\\nBatch={bs}  Steps={NUM_STEPS}  Seeds=[{seeds_str}]',\n",
    "                 fontsize=12, fontweight=\"bold\", y=0.98, va=\"top\")\n",
    "\n",
    "    # ── Image rows ──\n",
    "    for row, key in enumerate(selected_keys):\n",
    "        imgs = image_store[key][prompt_idx]\n",
    "        m = all_results[key][prompt_idx]\n",
    "        parts = key.split(\"/\")\n",
    "        mname, method = parts[0], parts[1]\n",
    "        color = COLORS.get(method, \"gray\")\n",
    "\n",
    "        # Left label cell\n",
    "        ax_label = fig.add_subplot(gs[row, 0])\n",
    "        ax_label.axis(\"off\")\n",
    "        label_text = (f\"{mname}\\n\"\n",
    "                      f\"{method}\\n\\n\"\n",
    "                      f\"LPIPS: {m['lpips_diversity']:.3f}\\n\"\n",
    "                      f\"CLIP:  {m['clip_score']:.3f}\\n\"\n",
    "                      f\"IR:    {m['image_reward']:.3f}\")\n",
    "        ax_label.text(0.95, 0.5, label_text, transform=ax_label.transAxes,\n",
    "                      fontsize=8, fontweight=\"bold\", color=color,\n",
    "                      va=\"center\", ha=\"right\", family=\"monospace\",\n",
    "                      bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\",\n",
    "                                edgecolor=color, alpha=0.9))\n",
    "\n",
    "        # Image cells\n",
    "        for col in range(bs):\n",
    "            ax = fig.add_subplot(gs[row, col + 1])\n",
    "            ax.imshow(imgs[col], interpolation=\"lanczos\")\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_edgecolor(color)\n",
    "                spine.set_linewidth(2)\n",
    "\n",
    "    plt.savefig(output_dir / f\"grid_{prompt_idx:03d}.png\",\n",
    "                dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for i in range(min(3, len(PROMPTS))):\n",
    "    show_grid(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Scale Ablation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Bar chart comparing the 4 runs across all metrics ──\n",
    "\n",
    "keys = list(all_results.keys())\n",
    "metric_names = [\"lpips_diversity\", \"clip_score\", \"clip_diversity\", \"image_reward\"]\n",
    "metric_labels = [\"LPIPS Diversity ↑\", \"CLIP Score ↑\", \"CLIP Diversity ↑\", \"ImageReward ↑\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 5))\n",
    "x = np.arange(len(keys))\n",
    "\n",
    "for ax, mname, mlabel in zip(axes, metric_names, metric_labels):\n",
    "    vals = [np.mean([r[mname] for r in all_results[k]]) for k in keys]\n",
    "    parts_list = [k.split(\"/\") for k in keys]\n",
    "    colors = [COLORS.get(p[1], \"gray\") for p in parts_list]\n",
    "    hatches = [\"\" if p[0] == \"stable-diffusion-v1-5\" else \"//\" for p in parts_list]\n",
    "\n",
    "    bars = ax.bar(x, vals, color=colors, alpha=0.85)\n",
    "    for bar, h in zip(bars, hatches):\n",
    "        bar.set_hatch(h)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([k.replace(\"/\", \"\\n\") for k in keys], fontsize=6)\n",
    "    ax.set_ylabel(mlabel)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "fig.suptitle(\"Metric Comparison Across All 4 Runs (hatched = DreamShaper-8)\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"metric_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early-Stop Ratio Ablation\n",
    "\n",
    "This ablation studies the effect of `early_stop_ratio` (k) on diversity and quality for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABLATION_RATIOS  = [0.1, 0.2, 0.5, 1.0]\n",
    "ABLATION_SCALE   = 10.0\n",
    "ABLATION_CFG     = 7.0\n",
    "ABLATION_MODEL   = \"Lykon/dreamshaper-8\"\n",
    "ABLATION_PROMPTS = PROMPTS[:5]\n",
    "\n",
    "ratio_results = {}\n",
    "\n",
    "print(f\"Early-Stop Ablation on {ABLATION_MODEL}\")\n",
    "abl_pipeline = DiversityGuidedPipeline(ABLATION_MODEL, device=device)\n",
    "\n",
    "for ratio in ABLATION_RATIOS:\n",
    "    key = f\"clean_estimate/k={ratio}\"\n",
    "    print(f\"\\n--- {key} ---\")\n",
    "    res = []\n",
    "    for pi, prompt in enumerate(ABLATION_PROMPTS):\n",
    "        out = abl_pipeline.generate(\n",
    "            prompt=prompt, batch_size=BATCH_SIZE,\n",
    "            num_inference_steps=NUM_STEPS,\n",
    "            guidance_scale=ABLATION_CFG,\n",
    "            diversity_scale=ABLATION_SCALE,\n",
    "            early_stop_ratio=ratio,\n",
    "            method=\"clean_estimate\",\n",
    "            loss_type=LOSS_TYPE,\n",
    "            seeds=SEEDS, height=HEIGHT, width=WIDTH,\n",
    "        )\n",
    "        m = metrics.evaluate_batch(out[\"images\"], prompt)\n",
    "        res.append(m)\n",
    "        print(f\"  [{pi+1}] LPIPS={m['lpips_diversity']:.4f} \"\n",
    "              f\"CLIP={m['clip_score']:.4f} IR={m['image_reward']:.4f}\")\n",
    "    ratio_results[key] = res\n",
    "\n",
    "del abl_pipeline\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "ratios = ABLATION_RATIOS\n",
    "lpips_vals = [np.mean([r[\"lpips_diversity\"] for r in ratio_results[f\"clean_estimate/k={k}\"]]) for k in ratios]\n",
    "clip_vals  = [np.mean([r[\"clip_score\"]      for r in ratio_results[f\"clean_estimate/k={k}\"]]) for k in ratios]\n",
    "ir_vals    = [np.mean([r[\"image_reward\"]    for r in ratio_results[f\"clean_estimate/k={k}\"]]) for k in ratios]\n",
    "\n",
    "axes[0].plot(ratios, lpips_vals, \"-o\", color=\"#2ca02c\")\n",
    "axes[0].set_xlabel(\"early_stop_ratio (k)\"); axes[0].set_ylabel(\"LPIPS Diversity\")\n",
    "axes[0].set_title(\"Diversity vs. Early-Stop Ratio\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(ratios, clip_vals, \"-o\", color=\"#1f77b4\")\n",
    "axes[1].set_xlabel(\"early_stop_ratio (k)\"); axes[1].set_ylabel(\"CLIP Score\")\n",
    "axes[1].set_title(\"CLIP Alignment vs. Early-Stop Ratio\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(ratios, ir_vals, \"-o\", color=\"#d62728\")\n",
    "axes[2].set_xlabel(\"early_stop_ratio (k)\"); axes[2].set_ylabel(\"ImageReward\")\n",
    "axes[2].set_title(\"ImageReward vs. Early-Stop Ratio\")\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle(f\"Early-Stop Ablation — {model_short(ABLATION_MODEL)} (λ={ABLATION_SCALE}, CFG={ABLATION_CFG})\",\n",
    "             fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"early_stop_ablation.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory if needed\n",
    "# del pipeline, metrics\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Done! All plots & images saved to:\", output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
