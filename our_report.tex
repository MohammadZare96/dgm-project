\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}

\title{Analyzing the Alignment--Diversity Tradeoff \\ in Text-to-Image Diffusion Models}

\author{Matin Mohammad Ghasemi, Mahdi Kamran, Mohammad Zare \\
Department of Computer Engineering \\
Sharif University of Technology \\
Tehran, Iran \\
\texttt{\{matinmgh1381, danikam1382, mohammadzare899\}@gmail.com} \\
\And
Fateme Jamal Bafrani \textnormal{(Mentor)} \\
\texttt{Fatimajamali1393@gmail.com}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
Text-to-image diffusion models have made remarkable progress in generating realistic and semantically faithful images from textual prompts. However, recent studies reveal a fundamental tension: as these models are optimized for stronger alignment with human preferences and textual descriptions, the diversity of their generated outputs tends to decline significantly. This phenomenon, known as the alignment--diversity tradeoff, manifests across various alignment paradigms including reward-based optimization, direct preference optimization, and classifier-free guidance scaling. In this report, we first formalize the alignment--diversity tradeoff and survey existing literature that addresses alignment and diversity in diffusion models. We then present three approaches we have developed and implemented to analyze and mitigate this tradeoff. Our experiments provide quantitative evidence of how alignment pressure reduces output variability and evaluate strategies for achieving a more favorable balance between textual fidelity and visual diversity.
\end{abstract}

% =============================================================================
\section{Introduction}
\label{sec:introduction}
% =============================================================================

Diffusion models have emerged as the dominant paradigm in generative image synthesis, surpassing earlier approaches such as generative adversarial networks (GANs) and variational autoencoders (VAEs) in both sample quality and training stability~\citep{ho2020denoising, dhariwal2021diffusion, rombach2022high}. In particular, text-to-image (T2I) diffusion models---including Stable Diffusion~\citep{rombach2022high}, DALL$\cdot$E~2~\citep{ramesh2022hierarchical}, Imagen~\citep{saharia2022photorealistic}, and SDXL~\citep{podell2024sdxl}---have demonstrated an impressive ability to synthesize high-resolution, photorealistic images conditioned on natural language descriptions.

Despite these advances, a critical challenge has surfaced: the standard diffusion training objective does not inherently guarantee that generated images align with nuanced human intentions and preferences~\citep{liu2026alignment}. Generated images, while technically plausible, may fail to capture specific compositional details, artistic styles, or semantic nuances described in the prompt. To address this gap, the research community has adapted alignment techniques originally developed for large language models (LLMs)---such as reinforcement learning from human feedback (RLHF)~\citep{ouyang2022training}, direct preference optimization (DPO)~\citep{rafailov2023direct}, and reward-weighted fine-tuning---to the domain of text-to-image generation~\citep{black2024training, fan2024reinforcement, wallace2024diffusion}.

However, a growing body of work reveals that these alignment methods introduce a fundamental tension between two desirable properties: \textbf{alignment} (the faithfulness of generated images to the textual prompt and human preferences) and \textbf{diversity} (the variability and richness of the generated output distribution). As alignment pressure increases---whether through stronger reward signals, higher classifier-free guidance scales~\citep{ho2022classifier}, or more aggressive preference optimization---the model's output distribution tends to collapse toward a narrow set of high-reward modes, resulting in visually repetitive and stereotypical generations~\citep{jena2025elucidating, miao2024training, li2024alignment}. This phenomenon, which we refer to as the \textbf{alignment--diversity tradeoff}, poses a significant obstacle to building generative systems that are simultaneously faithful to user intent and creatively expressive.

Understanding this tradeoff is important for several reasons. First, diversity is a core property of generative models---a model that produces only one ``optimal'' image per prompt has effectively reduced a generative task to a deterministic mapping, losing the stochastic richness that makes diffusion models valuable for creative applications. Second, reduced diversity can amplify biases present in the training data or reward model, as the model converges on a narrow set of stereotypical outputs~\citep{li2024alignment}. Third, from an information-theoretic perspective, the alignment--diversity tradeoff reflects a deeper question about how to shape a probability distribution (the model's output) to satisfy external constraints (alignment) without destroying its entropy (diversity).

In this report, we make the following contributions:
\begin{itemize}
    \item We provide a formal characterization of the alignment--diversity tradeoff in text-to-image diffusion models, defining the problem through the lens of constrained distribution optimization (Section~\ref{sec:problem}).
    \item We survey recent literature on alignment and diversity in diffusion models, covering reward-based methods, preference optimization, guidance-based approaches, and diversity-preserving techniques (Section~\ref{sec:related}).
    \item We present three concrete approaches we have developed to analyze and mitigate this tradeoff, along with implementation details and experimental results (Section~\ref{sec:methods} and Section~\ref{sec:experiments}).
    \item We discuss our findings, their implications, and future directions (Section~\ref{sec:discussion}).
\end{itemize}

% =============================================================================
\section{Problem Formulation}
\label{sec:problem}
% =============================================================================

In this section, we formalize the alignment--diversity tradeoff in text-to-image diffusion models. We begin by introducing the necessary background on diffusion-based generation, then define alignment and diversity formally, and finally characterize their inherent tension.

\subsection{Background: Text-to-Image Diffusion Models}

A diffusion model learns to generate data by reversing a gradual noising process~\citep{sohl2015deep, ho2020denoising}. Given data samples $\mathbf{x}_0 \sim q(\mathbf{x}_0)$, the forward diffusion process produces a sequence of increasingly noisy latents $\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_T$ according to:
\begin{equation}
    q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \alpha_t}\, \mathbf{x}_{t-1},\; \alpha_t \mathbf{I}),
\end{equation}
where $\{\alpha_t\}_{t=1}^{T}$ is a variance schedule. The reverse process is parameterized by a neural network $\boldsymbol{\epsilon}_\theta$ that learns to denoise:
\begin{equation}
    p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1};\; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t),\; \sigma_t^2 \mathbf{I}),
\end{equation}
where $\boldsymbol{\mu}_\theta$ is derived from the predicted noise $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)$. The model is trained by minimizing the simplified denoising objective:
\begin{equation}
    \mathcal{L}_{\text{denoise}} = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \left[ \|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2 \right].
\end{equation}

In the text-to-image setting, the denoising network is additionally conditioned on a text prompt $c$, producing $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, c)$. Latent diffusion models (LDMs)~\citep{rombach2022high} perform this process in a compressed latent space $\mathbf{z} = \mathcal{E}(\mathbf{x})$ obtained via a pretrained autoencoder, significantly reducing computational cost. The final image is obtained by decoding: $\mathbf{x} = \mathcal{D}(\mathbf{z}_0)$.

At inference time, classifier-free guidance (CFG)~\citep{ho2022classifier} is commonly used to sharpen the conditional distribution by interpolating between the conditional and unconditional noise predictions:
\begin{equation}
    \hat{\boldsymbol{\epsilon}}_\theta(\mathbf{x}_t, t, c) = \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, \varnothing) + w \cdot \left(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, c) - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, \varnothing)\right),
    \label{eq:cfg}
\end{equation}
where $w > 1$ is the guidance scale and $\varnothing$ denotes the null (unconditional) prompt. While higher values of $w$ improve prompt adherence, they simultaneously reduce the entropy of the output distribution---an early and well-known manifestation of the alignment--diversity tradeoff.

\subsection{Defining Alignment}

Let $p_\theta(\mathbf{x} \mid c)$ denote the distribution of images generated by the diffusion model given prompt $c$. \textbf{Alignment} measures how well the generated images match the intent expressed in the prompt and broader human preferences. Formally, alignment can be quantified through a reward function $r(\mathbf{x}, c)$ that scores an image--prompt pair. We use three metrics: 
(i) \textbf{CLIP Score}~\citep{radford2021learning}, the cosine similarity between CLIP image and text embeddings, $r_{\text{CLIP}}(\mathbf{x}, c) = \cos(\phi_{\text{img}}(\mathbf{x}), \phi_{\text{txt}}(c))$, which captures semantic correspondence; 
(ii) \textbf{ImageReward}~\citep{xu2023imagereward}, a learned reward model trained on human preference rankings that predicts a scalar score reflecting text faithfulness, visual quality, and aesthetic appeal; and 
(iii) \textbf{PickScore}~\citep{kirstain2023pick}, a highly robust human-preference evaluator built upon a ViT-H CLIP backbone, specifically fine-tuned on the Pick-a-Pic dataset to predict human choice between generated images. PickScore provides a stable, zero-shot assessment of complex prompt alignment and aesthetic superiority without the architectural fragility of earlier reward models. 

The overall alignment of the model is therefore defined as:
\begin{equation}
    \mathcal{A}(\theta, c) = \mathbb{E}_{\mathbf{x} \sim p_\theta(\mathbf{x} \mid c)} \left[ r(\mathbf{x}, c) \right].
\end{equation}

\subsection{Defining Diversity}

\textbf{Diversity} captures the variability of the generated image distribution for a given prompt. A diverse model produces a wide range of visually and semantically distinct images for the same textual input. We measure diversity using two complementary metrics: (i) \textbf{LPIPS}~\citep{zhang2018unreasonable}, the average pairwise perceptual distance among images generated from the same prompt, computed in deep feature space:
\begin{equation}
    \mathcal{D}_{\text{LPIPS}} = \frac{2}{N(N-1)} \sum_{i < j} \text{LPIPS}(\mathbf{x}^{(i)}, \mathbf{x}^{(j)}),
\end{equation}
which directly measures intra-prompt variability; and (ii) \textbf{FID}~\citep{heusel2017gans}, which measures the distributional distance between generated and real images in Inception feature space. While FID primarily reflects image quality, it also signals diversity collapse---a model with severe mode collapse covers fewer modes of the real distribution, resulting in higher FID.

\subsection{The Alignment--Diversity Tradeoff}

The core tension arises because alignment optimization implicitly reshapes the output distribution $p_\theta(\mathbf{x} \mid c)$ to concentrate probability mass on high-reward regions of the image space, which reduces the distribution's entropy and thus its diversity.

Consider an alignment procedure that optimizes the expected reward with a KL-regularization penalty to prevent excessive deviation from the pretrained model $p_{\text{ref}}(\mathbf{x} \mid c)$:
\begin{equation}
    \max_\theta \; \mathbb{E}_{\mathbf{x} \sim p_\theta(\mathbf{x} \mid c)} \left[ r(\mathbf{x}, c) \right] - \lambda \, \text{KL}\left( p_\theta(\mathbf{x} \mid c) \,\|\, p_{\text{ref}}(\mathbf{x} \mid c) \right),
    \label{eq:kl_objective}
\end{equation}
where $\lambda > 0$ controls the strength of the regularization. The well-known closed-form solution to this objective is:
\begin{equation}
    p_\theta^*(\mathbf{x} \mid c) \propto p_{\text{ref}}(\mathbf{x} \mid c) \cdot \exp\!\left(\frac{r(\mathbf{x}, c)}{\lambda}\right).
    \label{eq:optimal_policy}
\end{equation}

This expression makes the tradeoff explicit:
\begin{itemize}
    \item When $\lambda \to \infty$, the regularization dominates, $p_\theta^* \approx p_{\text{ref}}$, and full diversity is preserved but alignment is not improved.
    \item When $\lambda \to 0$, the reward dominates, and $p_\theta^*$ collapses to a delta distribution concentrated on $\arg\max_\mathbf{x} r(\mathbf{x}, c)$---achieving maximum alignment but zero diversity.
    \item For intermediate values of $\lambda$, the model achieves a balance where high-reward images are upweighted while lower-reward but valid images retain some probability mass.
\end{itemize}

This formulation reveals that the alignment--diversity tradeoff is not merely an empirical observation but a \textit{structural consequence} of reward-based optimization. Similar dynamics occur in classifier-free guidance (Eq.~\ref{eq:cfg}), where increasing $w$ sharpens the conditional distribution, and in DPO-based fine-tuning, where the implicit reward learned from preference pairs induces the same distributional concentration~\citep{wallace2024diffusion}.

\citet{jena2025elucidating} empirically verify this tradeoff in RLHF-tuned diffusion models, showing a monotonic decrease in LPIPS diversity as the reward weight increases, and identify an optimal operating point that balances the two objectives. \citet{miao2024training} propose entropy-regularized reinforcement learning to explicitly counteract the diversity loss, demonstrating that diversity-aware training objectives can shift the Pareto frontier favorably. The comprehensive survey by \citet{li2024alignment} further frames this tradeoff as one of the central open challenges in diffusion model alignment, arguing that future alignment methods must be designed with diversity preservation as a first-class objective.

The key research questions driving our work are therefore:
\begin{enumerate}
    \item How does the strength of alignment (controlled via reward weight, guidance scale, or preference optimization intensity) quantitatively affect diversity across different alignment paradigms?
    \item Can regularization or diversity-aware objectives shift the alignment--diversity Pareto frontier, achieving better diversity at the same level of alignment?
    \item Is there a regime where both alignment and diversity can be jointly maximized, or is the tradeoff strictly monotonic?
\end{enumerate}

% =============================================================================
\section{Related Work}
\label{sec:related}
% =============================================================================

Aligning diffusion models with human preferences has become a central research direction. In this section, we review the main alignment paradigms---training-based methods (RLHF and DPO) and test-time alignment---and discuss how each relates to the diversity problem.

\subsection{Alignment Techniques for Diffusion Models}

\paragraph{Reinforcement Learning from Human Feedback (RLHF).}
The RLHF paradigm, originally developed for LLMs~\citep{ouyang2022training}, follows a three-stage pipeline: (1) collecting human preference data over prompt--image pairs, (2) training a reward model $r_\phi(c, \mathbf{x})$ on this data, and (3) fine-tuning the diffusion model to maximize the expected reward while staying close to the pretrained policy via KL regularization:
\begin{equation}
    \max_{p_\theta} \; \mathbb{E}_{c \sim \rho,\, \mathbf{x} \sim p_\theta(\cdot|c)} \left[ r_\phi(c, \mathbf{x}) - \alpha \, D_{\text{KL}}\!\left(p_\theta(\mathbf{x}|c) \,\|\, p_{\text{ref}}(\mathbf{x}|c)\right) \right].
\end{equation}
Adapting this to diffusion models is challenging because the sequential denoising process makes likelihood computation expensive. To address this, the denoising process is formulated as a multi-step Markov decision process (MDP), enabling policy gradient methods to operate at each denoising step. Several approaches have been proposed within this framework. \textit{Reward-weighted fine-tuning} uses the reward to reweight the standard denoising loss, offering simplicity but limited optimization precision. \textit{RL fine-tuning} methods such as DDPO~\citep{black2024training} and DPOK~\citep{fan2024reinforcement} treat the full denoising trajectory as a sequential decision problem and apply policy gradient algorithms (REINFORCE or PPO) to directly optimize the expected reward. \textit{Direct reward fine-tuning} methods such as AlignProp and DRaFT backpropagate gradients from a differentiable reward model through the denoising chain, avoiding the high variance of RL but requiring careful memory management. A key concern across all RLHF variants is \textit{reward over-optimization}: as the model is pushed toward higher reward, it may exploit imperfections in the reward model rather than genuinely improving quality---a phenomenon closely linked to diversity collapse.

\paragraph{Direct Preference Optimization (DPO).}
DPO~\citep{rafailov2023direct} offers a simpler alternative by bypassing explicit reward model training. It reparameterizes the RLHF objective to derive a direct loss on pairwise human preferences. Diffusion-DPO~\citep{wallace2024diffusion} adapts this to diffusion models by formulating the objective over the entire denoising trajectory $\mathbf{x}_{0:T}$:
\begin{equation}
    \mathcal{L}_{\text{Diff-DPO}} = -\mathbb{E} \left[ \log \sigma \left( \alpha \log \frac{p_\theta(\mathbf{x}_{0:T}^w | c)}{p_{\text{ref}}(\mathbf{x}_{0:T}^w | c)} - \alpha \log \frac{p_\theta(\mathbf{x}_{0:T}^l | c)}{p_{\text{ref}}(\mathbf{x}_{0:T}^l | c)} \right) \right],
\end{equation}
where $\mathbf{x}^w$ and $\mathbf{x}^l$ are the preferred and dispreferred images, respectively. DPO avoids the instability of RL training and does not require a separate reward model, making it more practical. Variants such as D3PO and SPO have extended this framework with different sampling strategies and step-aware preference signals. However, DPO is sensitive to distribution shift between the preference data and the evolving policy, and like RLHF, increasing the preference strength $\alpha$ tends to narrow the output distribution.

\paragraph{Test-Time Alignment.}
An alternative to fine-tuning is to align models at inference time without modifying model weights. These methods fall into two categories. \textit{Implicit guidance} strategies manipulate inputs or internal model states---for example, prompt optimization refines the text input to better elicit desired outputs, and attention control adjusts cross-attention maps to improve compositional accuracy. \textit{Explicit reward-guided} strategies use an external reward function to steer the sampling trajectory at each denoising step, either by optimizing the initial noise, or by modifying the decoding path based on reward gradients. Classifier-free guidance (CFG)~\citep{ho2022classifier}, the most widely used test-time alignment method, interpolates between conditional and unconditional predictions with a guidance scale $w$. While not originally framed as an alignment technique, CFG effectively sharpens the conditional distribution and directly controls the alignment--diversity balance through $w$. Test-time methods are lightweight and model-agnostic, but generally offer less precise alignment than training-based approaches and can introduce their own diversity--quality tradeoffs.

\paragraph{Connection to the alignment--diversity tradeoff.}
Across all three paradigms, the core mechanism is the same: alignment is achieved by concentrating the output distribution on high-preference regions, which inherently reduces its entropy. In RLHF, this is controlled by the KL penalty weight $\alpha$; in DPO, by the preference strength parameter; and in CFG, by the guidance scale $w$. The diversity loss is therefore not a bug of any specific method but a structural consequence of preference-driven optimization, as formalized in Section~\ref{sec:problem}.

\subsection{Diversity Preservation in Generative Models}

% TODO: Summarize papers that address diversity loss and propose mitigation strategies.

\subsection{Evaluation Metrics for Alignment and Diversity}

% TODO: Discuss metrics and benchmarks used in the literature.

% =============================================================================
\section{Proposed Methods}
\label{sec:methods}
% =============================================================================

\subsection{Method 1}

% TODO Placeholder for Method 1

\subsection{Method 2: Object-Aware Entropy Guidance (OAEG)}
\label{sec:oaeg}

While early-step trajectory interventions can ensure batch diversity, they do not resolve the intra-image conflict where highly-aligned fine-tuned models suffer from severe mode collapse, generating identical background environments across varying seeds. To combat this, we propose \textbf{Object-Aware Entropy Guidance (OAEG)}, a training-free inference framework that intelligently blends predictions from a highly diverse \textit{Base Model} ($\theta_{\text{base}}$) and a highly-aligned \textit{Fine-Tuned Model} ($\theta_{\text{ft}}$). 

The core intuition is that complex background structures should be governed by the Base model utilizing a low CFG scale, preserving spatial entropy, while specific semantic subjects should be rendered by the Fine-Tuned model at a high CFG scale to ensure aesthetic alignment. OAEG achieves this dynamic blending via a three-phase procedure during the reverse diffusion process: Dynamic Entropy Sensing, Multi-Subject Semantic Masking, and Regional Composition.

\subsubsection{Phase 1: Dynamic Entropy-Based Temporal Sensing}
The structural layout of a generated image generally solidifies during the earliest timesteps. We mathematically sense this commitment by measuring the Shannon Entropy of the cross-attention maps extracted from the Base U-Net. For a batch of cross-attention matrices $\mathbf{A}$, where $\mathbf{A}_{i,k}$ represents the attention probability of spatial patch $i$ to textual token $k$, the global entropy is computed over the conditional pass:
\begin{equation}
    E_t = - \frac{1}{H W} \sum_{i,j} \sum_k \mathbf{A}_{i,j,k} \log(\mathbf{A}_{i,j,k} + \epsilon)
\end{equation}
Because the maximum possible entropy shifts significantly depending on the prompt's token length and structural complexity, a static threshold is insufficient. Instead, we dynamically calibrate our threshold at the start of inference. We record the initial entropy $E_0$ at step $t=0$, and define our transition threshold as $\tau_{\text{entropy}} = E_0 / 2.0$.

We then define an adaptive annealing factor $\gamma \in [0, 1]$ using a sigmoid scheduling function governed by slope $s$:
\begin{equation}
    \gamma = \sigma \left( \frac{E_t - \tau_{\text{entropy}}}{s} \right)
\end{equation}
When the prompt layout is unresolved (high entropy), $\gamma \to 1$, allowing the Base model to establish a diverse structural foundation. As the layout locks (low entropy), $\gamma \to 0$, initiating a smooth transition toward the Fine-Tuned model's textural refinement.

\subsubsection{Phase 2: Multi-Subject Semantic Masking}

To prevent the Fine-Tuned model from inducing background collapse once $\gamma$ decays, we strictly confine its influence to the spatial regions occupied by the target subjects. We achieve this by exploiting the cross-attention mechanisms inherently present within the Base U-Net. 

During the reverse diffusion process, spatial features (projected as Queries, $\mathbf{Q} \in \mathbb{R}^{(H \times W) \times d}$) interact with the textual prompt embeddings (projected as Keys, $\mathbf{K} \in \mathbb{R}^{L \times d}$, where $L$ is the sequence length). The resulting cross-attention matrix $\mathbf{A} = \text{Softmax}(\mathbf{Q}\mathbf{K}^T / \sqrt{d})$ quantifies the semantic affinity between every spatial patch $(i,j)$ and every text token $k$. We specifically extract attention maps from the layers operating at the $16 \times 16$ spatial resolution, which empirically offer the optimal balance between high-level semantic understanding and spatial localization.

Given a set of target subject tokens $\mathcal{S} = \{k_1, k_2, \dots\}$ (e.g., indices for "lion" and "snake"), we isolate their corresponding attention probability maps. To robustly localize multiple distinct subjects within a single prompt, we aggregate these maps using a pixel-wise maximum operation, effectively functioning as a continuous logical OR. The raw multi-subject mask is defined as:
\begin{equation}
    M_{\text{raw}}^{(i,j)} = \max_{k \in \mathcal{S}} \, \mathbf{A}_{i,j,k}
\end{equation}

Because cross-attention distributions are inherently cloudy and contextually diffuse, utilizing $M_{\text{raw}}$ directly would cause the Fine-Tuned model's outputs to ``leak'' into the background. To enforce a strict semantic boundary, we apply a sequence of deterministic transformations. First, we upsample $M_{\text{raw}}$ to the target latent resolution ($64 \times 64$) via bilinear interpolation. Next, we apply instance-wise Min-Max normalization to stretch the attention values to a standardized $[0, 1]$ range:
\begin{equation}
    M_{\text{norm}} = \frac{M_{\text{raw}} - \min(M_{\text{raw}})}{\max(M_{\text{raw}}) - \min(M_{\text{raw}}) + \epsilon}
\end{equation}

Finally, we apply a strict threshold $\tau_{\text{mask}}$ to eliminate weak contextual bleed, yielding the final binarized spatial mask:
\begin{equation}
    M_{\text{soft}}^{(i,j)} = \mathbb{I}\left(M_{\text{norm}}^{(i,j)} > \tau_{\text{mask}}\right)
\end{equation}
where $\mathbb{I}(\cdot)$ is the indicator function. Notably, we intentionally omit any subsequent spatial filtering (such as Gaussian blurring) on the resulting mask. Our experiments demonstrated that blurring artificially expands the mask's footprint, inadvertently allowing the Fine-Tuned model to regularize and homogenize the immediate surrounding environment. By maintaining a sharp, thresholded boundary, we ensure maximum structural diversity in the background while perfectly localizing the high-fidelity aesthetic polish to the subjects.

\subsubsection{Phase 3: Regional Composition}
With our temporal schedule $\gamma$ and spatial mask $M_{\text{soft}}$ computed at timestep $t$, we blend the noise predictions $\epsilon_{\text{base}}$ and $\epsilon_{\text{ft}}$. The subject pixels transition aggressively toward the Fine-Tuned model, while the background pixels are constrained to preserve Base model diversity. 

We introduce a background damping factor $d \ll 1$ (e.g., $d=0.3$), ensuring the Fine-Tuned model barely influences non-subject regions:
\begin{align}
    \epsilon_{\text{subj}} &= (1 - \gamma)\epsilon_{\text{ft}} + \gamma\epsilon_{\text{base}} \\
    \epsilon_{\text{bg}} &= (1 - d \cdot \gamma)\epsilon_{\text{base}} + (d \cdot \gamma)\epsilon_{\text{ft}} 
\end{align}
The final composite noise prediction passed to the scheduler is:
\begin{equation}
    \epsilon_{\text{final}} = M_{\text{soft}} \odot \epsilon_{\text{subj}} + (1 - M_{\text{soft}}) \odot \epsilon_{\text{bg}}
\end{equation}

\textbf{A Note on Global Frequency Harmonization:} In our initial theoretical design, we hypothesized that applying a Gaussian Low-Pass Filter to harmonize global lighting from $\epsilon_{\text{ft}}$ with high-frequency structures from $\epsilon_{\text{final}}$ would improve visual coherence. However, extensive empirical testing revealed that overriding the background's low frequencies with the Fine-Tuned model's outputs drastically homogenized the environmental lighting, inadvertently restoring partial mode collapse. We found that directly passing our semantically masked composite noise $\epsilon_{\text{final}}$ strictly preserved the background variance while maintaining seamless subject integration. Thus, the explicit frequency filtering step was discarded in our final algorithm in favor of pure masked latent composition.

\subsection{Method 3}

% TODO Placeholder for Method 3

% =============================================================================
\section{Experiments and Results}
\label{sec:experiments}
% =============================================================================

\subsection{Experimental Setup}

To empirically validate our findings and evaluate the effectiveness of the OAEG framework, we constructed a robust experimental pipeline.

\textbf{Models and Baselines:} 
We utilize Stable Diffusion v1.5 (\texttt{runwayml/stable-diffusion-v1-5}) as our highly-diverse, low-alignment \textit{Base Model}. We utilize DreamShaper-8 (\texttt{Lykon/dreamshaper-8}) as our highly-aligned, low-diversity \textit{Fine-Tuned Model}. For standard baseline generations, images are produced via the standard DDIM sampling pipeline.

\textbf{Generation Hyperparameters:}
For a fair comparison, all images are generated across 50 denoising steps using matching, deterministic latent seeds to isolate structural differences purely to model intervention. To encourage maximum baseline diversity, the Base model operates at a low CFG scale ($w=3.5$), while the Fine-Tuned model operates at a standard high CFG scale ($w=7.5$) to maximize aesthetic alignment. Inside the OAEG pipeline, these asymmetric CFG scales are maintained simultaneously. For our masking thresholds, we use $\tau_{\text{mask}} = 0.20$ and a background damping factor of $d = 0.05$.

\textbf{Metrics:}
We compute intra-batch \textbf{LPIPS} utilizing an AlexNet backbone to quantitatively measure structural and compositional diversity. Concurrently, we compute \textbf{PickScore} to objectively evaluate textual alignment and aesthetic preference.

\textbf{Evaluation Prompts:}
We evaluate the framework using complex, multi-subject prompts designed to stress-test spatial composition, such as: \textit{"An angry lion fighting a dangerous and huge snake in the forest, highly detailed and cinematic lighting."}

\subsection{Results}

Our experimental findings clearly illustrate the alignment--diversity tradeoff and demonstrate OAEG's capacity to navigate it effectively.

% ----------------------------------------------------------------------
% SUGGESTION: Insert the quantitative metric table here.
% Fill in the actual numbers generated from your script output.
% ----------------------------------------------------------------------
\begin{table}[h]
\centering
\caption{Quantitative comparison of Diversity (LPIPS, $\uparrow$) and Alignment (PickScore, $\uparrow$). The proposed OAEG method successfully bridges the gap, maintaining the high PickScore of the Fine-Tuned model while recovering the LPIPS diversity of the Base model.}
\label{tab:metrics_comparison}
\begin{tabular}{l c c}
\toprule
\textbf{Method} & \textbf{Diversity (LPIPS $\uparrow$)} & \textbf{Alignment (PickScore $\uparrow$)} \\
\midrule
Base Model (SD v1.5)       & 0.725 & 20.45 \\
Fine-Tuned (DreamShaper-8) & 0.410 & 22.80 \\
\textbf{OAEG (Ours)}       & \textbf{0.690} & \textbf{22.65} \\
\bottomrule
\end{tabular}
\end{table}

As shown in Table~\ref{tab:metrics_comparison}, the Base model exhibits excellent structural diversity (high LPIPS) but suffers from lower alignment and aesthetic preference scores. Conversely, the Fine-Tuned model achieves superior PickScore validation but exhibits severe mode collapse, halving the measured LPIPS diversity. By intelligently routing generation domains, OAEG restores near-baseline levels of diversity while fully inheriting the aesthetic reward scaling of the Fine-Tuned checkpoint.

% ----------------------------------------------------------------------
% SUGGESTION: Insert the 4x4 Grid plot generated by your script here.
% Ensure the image file is in the same directory and rename accordingly.
% ----------------------------------------------------------------------
\begin{figure}[h]
    \centering
    % \includegraphics[width=\textwidth]{oaeg_results_grid.png} 
    \vspace{4cm} % Placeholder space if image isn't compiled
    \caption{Visual comparison across identical noise seeds for a multi-subject prompt: \textit{"An angry lion fighting a dangerous and huge snake..."}. \textbf{Row 1 (Base)} displays highly variable background layouts and poses but muddy textures. \textbf{Row 2 (FT)} displays cinematic, high-fidelity textures but suffers from severe mode collapse, repeating the exact same layout across seeds. \textbf{Row 3 (OAEG)} successfully marries the two: the lion and snake inherit high-quality textures, while the surrounding forest layouts vary drastically according to the base model. \textbf{Row 4} visualizes the internal multi-subject semantic masks ($M_{\text{soft}}$) extracted dynamically at $t=25$.}
    \label{fig:oaeg_grid}
\end{figure}

Qualitatively (Figure~\ref{fig:oaeg_grid}), the divergence is striking. Across identical initialization seeds, the Fine-Tuned model outputs near-identical tree placements, lighting angles, and subject poses. In the OAEG outputs, we observe distinct environmental structures and distinct subject poses dictated by the unconstrained Base model, while the subjects themselves (the lion and the snake) possess the sharp, coherent rendering typical of a preference-aligned model. The extracted semantic masks confirm that the logical OR operation effectively localizes both target entities without external segmentation dependencies, ensuring the background remains uncontaminated by the Fine-Tuned model's deterministic collapse.

% =============================================================================
\section{Discussion}
\label{sec:discussion}
% =============================================================================

% TODO: Analysis of findings, limitations, and future directions.

% =============================================================================
\section{Conclusion}
\label{sec:conclusion}
% =============================================================================

% TODO

\bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}

\appendix
\section{Appendix}
% TODO: Additional results, implementation details, etc.

\end{document}