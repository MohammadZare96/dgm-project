\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}
\usepackage{float}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}

\title{Analyzing the Alignment--Diversity Tradeoff \\ in Text-to-Image Diffusion Models}

\author{Matin Mohammad Ghasemi, Mahdi Kamran, Mohammad Zare \\
Department of Computer Engineering \\
Sharif University of Technology \\
Tehran, Iran \\
\texttt{\{matinmgh1381, danikam1382, mohammadzare899\}@gmail.com} \\
\And
Fateme Jamal Bafrani \textnormal{(Mentor)} \\
\texttt{Fatimajamali1393@gmail.com}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
Text-to-image diffusion models have made remarkable progress in generating realistic and semantically faithful images from textual prompts. However, recent studies reveal a fundamental tension: as these models are optimized for stronger alignment with human preferences and textual descriptions, the diversity of their generated outputs tends to decline significantly. This phenomenon, known as the alignment--diversity tradeoff, manifests across various alignment paradigms including reward-based optimization, direct preference optimization, and classifier-free guidance scaling. In this report, we first formalize the alignment--diversity tradeoff and survey existing literature that addresses alignment and diversity in diffusion models. We then present three approaches we have developed and implemented to analyze and mitigate this tradeoff. Our experiments provide quantitative evidence of how alignment pressure reduces output variability and evaluate strategies for achieving a more favorable balance between textual fidelity and visual diversity.
\end{abstract}

% =============================================================================
\section{Introduction}
\label{sec:introduction}
% =============================================================================

Diffusion models have emerged as the dominant paradigm in generative image synthesis, surpassing earlier approaches such as generative adversarial networks (GANs) and variational autoencoders (VAEs) in both sample quality and training stability~\citep{ho2020denoising, dhariwal2021diffusion, rombach2022high}. In particular, text-to-image (T2I) diffusion models---including Stable Diffusion~\citep{rombach2022high}, DALL$\cdot$E~2~\citep{ramesh2022hierarchical}, Imagen~\citep{saharia2022photorealistic}, and SDXL~\citep{podell2024sdxl}---have demonstrated an impressive ability to synthesize high-resolution, photorealistic images conditioned on natural language descriptions.

Despite these advances, a critical challenge has surfaced: the standard diffusion training objective does not inherently guarantee that generated images align with nuanced human intentions and preferences~\citep{liu2026alignment}. Generated images, while technically plausible, may fail to capture specific compositional details, artistic styles, or semantic nuances described in the prompt. To address this gap, the research community has adapted alignment techniques originally developed for large language models (LLMs)---such as reinforcement learning from human feedback (RLHF)~\citep{ouyang2022training}, direct preference optimization (DPO)~\citep{rafailov2023direct}, and reward-weighted fine-tuning---to the domain of text-to-image generation~\citep{black2024training, fan2024reinforcement, wallace2024diffusion}.

However, a growing body of work reveals that these alignment methods introduce a fundamental tension between two desirable properties: \textbf{alignment} (the faithfulness of generated images to the textual prompt and human preferences) and \textbf{diversity} (the variability and richness of the generated output distribution). As alignment pressure increases---whether through stronger reward signals, higher classifier-free guidance scales~\citep{ho2022classifier}, or more aggressive preference optimization---the model's output distribution tends to collapse toward a narrow set of high-reward modes, resulting in visually repetitive and stereotypical generations~\citep{jena2025elucidating, miao2024training, li2024alignment}. This phenomenon, which we refer to as the \textbf{alignment--diversity tradeoff}, poses a significant obstacle to building generative systems that are simultaneously faithful to user intent and creatively expressive.

Understanding this tradeoff is important for several reasons. First, diversity is a core property of generative models---a model that produces only one ``optimal'' image per prompt has effectively reduced a generative task to a deterministic mapping, losing the stochastic richness that makes diffusion models valuable for creative applications. Second, reduced diversity can amplify biases present in the training data or reward model, as the model converges on a narrow set of stereotypical outputs~\citep{li2024alignment}. Third, from an information-theoretic perspective, the alignment--diversity tradeoff reflects a deeper question about how to shape a probability distribution (the model's output) to satisfy external constraints (alignment) without destroying its entropy (diversity).

In this report, we make the following contributions:
\begin{itemize}
    \item We provide a formal characterization of the alignment--diversity tradeoff in text-to-image diffusion models, defining the problem through the lens of constrained distribution optimization (Section~\ref{sec:problem}).
    \item We survey recent literature on alignment and diversity in diffusion models, covering reward-based methods, preference optimization, guidance-based approaches, and diversity-preserving techniques (Section~\ref{sec:related}).
    \item We present three concrete approaches we have developed to analyze and mitigate this tradeoff, along with implementation details and experimental results (Section~\ref{sec:methods} and Section~\ref{sec:experiments}).
    \item We discuss our findings, their implications, and future directions (Section~\ref{sec:discussion}).
\end{itemize}

% =============================================================================
\section{Problem Formulation}
\label{sec:problem}
% =============================================================================

In this section, we formalize the alignment--diversity tradeoff in text-to-image diffusion models. We begin by introducing the necessary background on diffusion-based generation, then define alignment and diversity formally, and finally characterize their inherent tension.

\subsection{Background: Text-to-Image Diffusion Models}

A diffusion model learns to generate data by reversing a gradual noising process~\citep{sohl2015deep, ho2020denoising}. Given data samples $\mathbf{x}_0 \sim q(\mathbf{x}_0)$, the forward diffusion process produces a sequence of increasingly noisy latents $\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_T$ according to:
\begin{equation}
    q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \alpha_t}\, \mathbf{x}_{t-1},\; \alpha_t \mathbf{I}),
\end{equation}
where $\{\alpha_t\}_{t=1}^{T}$ is a variance schedule. The reverse process is parameterized by a neural network $\boldsymbol{\epsilon}_\theta$ that learns to denoise:
\begin{equation}
    p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1};\; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t),\; \sigma_t^2 \mathbf{I}),
\end{equation}
where $\boldsymbol{\mu}_\theta$ is derived from the predicted noise $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)$. The model is trained by minimizing the simplified denoising objective:
\begin{equation}
    \mathcal{L}_{\text{denoise}} = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \left[ \|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2 \right].
\end{equation}

In the text-to-image setting, the denoising network is additionally conditioned on a text prompt $c$, producing $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, c)$. Latent diffusion models (LDMs)~\citep{rombach2022high} perform this process in a compressed latent space $\mathbf{z} = \mathcal{E}(\mathbf{x})$ obtained via a pretrained autoencoder, significantly reducing computational cost. The final image is obtained by decoding: $\mathbf{x} = \mathcal{D}(\mathbf{z}_0)$.

At inference time, classifier-free guidance (CFG)~\citep{ho2022classifier} is commonly used to sharpen the conditional distribution by interpolating between the conditional and unconditional noise predictions:
\begin{equation}
    \hat{\boldsymbol{\epsilon}}_\theta(\mathbf{x}_t, t, c) = \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, \varnothing) + w \cdot \left(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, c) - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, \varnothing)\right),
    \label{eq:cfg}
\end{equation}
where $w > 1$ is the guidance scale and $\varnothing$ denotes the null (unconditional) prompt. While higher values of $w$ improve prompt adherence, they simultaneously reduce the entropy of the output distribution---an early and well-known manifestation of the alignment--diversity tradeoff.

\subsection{Defining Alignment}

Let $p_\theta(\mathbf{x} \mid c)$ denote the distribution of images generated by the diffusion model given prompt $c$. \textbf{Alignment} measures how well the generated images match the intent expressed in the prompt and broader human preferences. Formally, alignment can be quantified through a reward function $r(\mathbf{x}, c)$ that scores an image--prompt pair. We use three metrics: 
(i) \textbf{CLIP Score}~\citep{radford2021learning}, the cosine similarity between CLIP image and text embeddings, $r_{\text{CLIP}}(\mathbf{x}, c) = \cos(\phi_{\text{img}}(\mathbf{x}), \phi_{\text{txt}}(c))$, which captures semantic correspondence; 
(ii) \textbf{ImageReward}~\citep{xu2023imagereward}, a learned reward model trained on human preference rankings that predicts a scalar score reflecting text faithfulness, visual quality, and aesthetic appeal; and 
(iii) \textbf{PickScore}~\citep{kirstain2023pick}, a highly robust human-preference evaluator built upon a ViT-H CLIP backbone, specifically fine-tuned on the Pick-a-Pic dataset to predict human choice between generated images. PickScore provides a stable, zero-shot assessment of complex prompt alignment and aesthetic superiority without the architectural fragility of earlier reward models. 

The overall alignment of the model is therefore defined as:
\begin{equation}
    \mathcal{A}(\theta, c) = \mathbb{E}_{\mathbf{x} \sim p_\theta(\mathbf{x} \mid c)} \left[ r(\mathbf{x}, c) \right].
\end{equation}

\subsection{Defining Diversity}

\textbf{Diversity} captures the variability of the generated image distribution for a given prompt. A diverse model produces a wide range of visually and semantically distinct images for the same textual input. We measure diversity using two complementary metrics: (i) \textbf{LPIPS}~\citep{zhang2018unreasonable}, the average pairwise perceptual distance among images generated from the same prompt, computed in deep feature space:
\begin{equation}
    \mathcal{D}_{\text{LPIPS}} = \frac{2}{N(N-1)} \sum_{i < j} \text{LPIPS}(\mathbf{x}^{(i)}, \mathbf{x}^{(j)}),
\end{equation}
which directly measures intra-prompt variability; and (ii) \textbf{FID}~\citep{heusel2017gans}, which measures the distributional distance between generated and real images in Inception feature space. While FID primarily reflects image quality, it also signals diversity collapse---a model with severe mode collapse covers fewer modes of the real distribution, resulting in higher FID.

\subsection{The Alignment--Diversity Tradeoff}

The core tension arises because alignment optimization implicitly reshapes the output distribution $p_\theta(\mathbf{x} \mid c)$ to concentrate probability mass on high-reward regions of the image space, which reduces the distribution's entropy and thus its diversity.

Consider an alignment procedure that optimizes the expected reward with a KL-regularization penalty to prevent excessive deviation from the pretrained model $p_{\text{ref}}(\mathbf{x} \mid c)$:
\begin{equation}
    \max_\theta \; \mathbb{E}_{\mathbf{x} \sim p_\theta(\mathbf{x} \mid c)} \left[ r(\mathbf{x}, c) \right] - \lambda \, \text{KL}\left( p_\theta(\mathbf{x} \mid c) \,\|\, p_{\text{ref}}(\mathbf{x} \mid c) \right),
    \label{eq:kl_objective}
\end{equation}
where $\lambda > 0$ controls the strength of the regularization. The well-known closed-form solution to this objective is:
\begin{equation}
    p_\theta^*(\mathbf{x} \mid c) \propto p_{\text{ref}}(\mathbf{x} \mid c) \cdot \exp\!\left(\frac{r(\mathbf{x}, c)}{\lambda}\right).
    \label{eq:optimal_policy}
\end{equation}

This expression makes the tradeoff explicit:
\begin{itemize}
    \item When $\lambda \to \infty$, the regularization dominates, $p_\theta^* \approx p_{\text{ref}}$, and full diversity is preserved but alignment is not improved.
    \item When $\lambda \to 0$, the reward dominates, and $p_\theta^*$ collapses to a delta distribution concentrated on $\arg\max_\mathbf{x} r(\mathbf{x}, c)$---achieving maximum alignment but zero diversity.
    \item For intermediate values of $\lambda$, the model achieves a balance where high-reward images are upweighted while lower-reward but valid images retain some probability mass.
\end{itemize}

This formulation reveals that the alignment--diversity tradeoff is not merely an empirical observation but a \textit{structural consequence} of reward-based optimization. Similar dynamics occur in classifier-free guidance (Eq.~\ref{eq:cfg}), where increasing $w$ sharpens the conditional distribution, and in DPO-based fine-tuning, where the implicit reward learned from preference pairs induces the same distributional concentration~\citep{wallace2024diffusion}.

\citet{jena2025elucidating} empirically verify this tradeoff in RLHF-tuned diffusion models, showing a monotonic decrease in LPIPS diversity as the reward weight increases, and identify an optimal operating point that balances the two objectives. \citet{miao2024training} propose entropy-regularized reinforcement learning to explicitly counteract the diversity loss, demonstrating that diversity-aware training objectives can shift the Pareto frontier favorably. The comprehensive survey by \citet{li2024alignment} further frames this tradeoff as one of the central open challenges in diffusion model alignment, arguing that future alignment methods must be designed with diversity preservation as a first-class objective.

The key research questions driving our work are therefore:
\begin{enumerate}
    \item How does the strength of alignment (controlled via reward weight, guidance scale, or preference optimization intensity) quantitatively affect diversity across different alignment paradigms?
    \item Can regularization or diversity-aware objectives shift the alignment--diversity Pareto frontier, achieving better diversity at the same level of alignment?
    \item Is there a regime where both alignment and diversity can be jointly maximized, or is the tradeoff strictly monotonic?
\end{enumerate}

% =============================================================================
\section{Related Work}
\label{sec:related}
% =============================================================================

Aligning diffusion models with human preferences has become a central research direction. In this section, we review the main alignment paradigms---training-based methods (RLHF and DPO) and test-time alignment---and discuss how each relates to the diversity problem.

\subsection{Alignment Techniques for Diffusion Models}

\paragraph{Reinforcement Learning from Human Feedback (RLHF).}
The RLHF paradigm, originally developed for LLMs~\citep{ouyang2022training}, follows a three-stage pipeline: (1) collecting human preference data over prompt--image pairs, (2) training a reward model $r_\phi(c, \mathbf{x})$ on this data, and (3) fine-tuning the diffusion model to maximize the expected reward while staying close to the pretrained policy via KL regularization:
\begin{equation}
    \max_{p_\theta} \; \mathbb{E}_{c \sim \rho,\, \mathbf{x} \sim p_\theta(\cdot|c)} \left[ r_\phi(c, \mathbf{x}) - \alpha \, D_{\text{KL}}\!\left(p_\theta(\mathbf{x}|c) \,\|\, p_{\text{ref}}(\mathbf{x}|c)\right) \right].
\end{equation}
Adapting this to diffusion models is challenging because the sequential denoising process makes likelihood computation expensive. To address this, the denoising process is formulated as a multi-step Markov decision process (MDP), enabling policy gradient methods to operate at each denoising step. Several approaches have been proposed within this framework. \textit{Reward-weighted fine-tuning} uses the reward to reweight the standard denoising loss, offering simplicity but limited optimization precision. \textit{RL fine-tuning} methods such as DDPO~\citep{black2024training} and DPOK~\citep{fan2024reinforcement} treat the full denoising trajectory as a sequential decision problem and apply policy gradient algorithms (REINFORCE or PPO) to directly optimize the expected reward. \textit{Direct reward fine-tuning} methods such as AlignProp and DRaFT backpropagate gradients from a differentiable reward model through the denoising chain, avoiding the high variance of RL but requiring careful memory management. A key concern across all RLHF variants is \textit{reward over-optimization}: as the model is pushed toward higher reward, it may exploit imperfections in the reward model rather than genuinely improving quality---a phenomenon closely linked to diversity collapse.

\paragraph{Direct Preference Optimization (DPO).}
DPO~\citep{rafailov2023direct} offers a simpler alternative by bypassing explicit reward model training. It reparameterizes the RLHF objective to derive a direct loss on pairwise human preferences. Diffusion-DPO~\citep{wallace2024diffusion} adapts this to diffusion models by formulating the objective over the entire denoising trajectory $\mathbf{x}_{0:T}$:
\begin{equation}
    \mathcal{L}_{\text{Diff-DPO}} = -\mathbb{E} \left[ \log \sigma \left( \alpha \log \frac{p_\theta(\mathbf{x}_{0:T}^w | c)}{p_{\text{ref}}(\mathbf{x}_{0:T}^w | c)} - \alpha \log \frac{p_\theta(\mathbf{x}_{0:T}^l | c)}{p_{\text{ref}}(\mathbf{x}_{0:T}^l | c)} \right) \right],
\end{equation}
where $\mathbf{x}^w$ and $\mathbf{x}^l$ are the preferred and dispreferred images, respectively. DPO avoids the instability of RL training and does not require a separate reward model, making it more practical. Variants such as D3PO and SPO have extended this framework with different sampling strategies and step-aware preference signals. However, DPO is sensitive to distribution shift between the preference data and the evolving policy, and like RLHF, increasing the preference strength $\alpha$ tends to narrow the output distribution.

\paragraph{Test-Time Alignment.}
An alternative to fine-tuning is to align models at inference time without modifying model weights. These methods fall into two categories. \textit{Implicit guidance} strategies manipulate inputs or internal model states---for example, prompt optimization refines the text input to better elicit desired outputs, and attention control adjusts cross-attention maps to improve compositional accuracy. \textit{Explicit reward-guided} strategies use an external reward function to steer the sampling trajectory at each denoising step, either by optimizing the initial noise, or by modifying the decoding path based on reward gradients. Classifier-free guidance (CFG)~\citep{ho2022classifier}, the most widely used test-time alignment method, interpolates between conditional and unconditional predictions with a guidance scale $w$. While not originally framed as an alignment technique, CFG effectively sharpens the conditional distribution and directly controls the alignment--diversity balance through $w$. Test-time methods are lightweight and model-agnostic, but generally offer less precise alignment than training-based approaches and can introduce their own diversity--quality tradeoffs.

\paragraph{Connection to the alignment--diversity tradeoff.}
Across all three paradigms, the core mechanism is the same: alignment is achieved by concentrating the output distribution on high-preference regions, which inherently reduces its entropy. In RLHF, this is controlled by the KL penalty weight $\alpha$; in DPO, by the preference strength parameter; and in CFG, by the guidance scale $w$. The diversity loss is therefore not a bug of any specific method but a structural consequence of preference-driven optimization, as formalized in Section~\ref{sec:problem}.

\subsection{Diversity Preservation in Generative Models}
Preserving or restoring diversity in conditional generation has been approached from multiple angles.

\paragraph{Batch-wise repulsion and particle-based sampling.}
Particle Guidance formulates joint sampling of multiple particles by adding the gradient of a global potential to the score function, producing repulsive forces between samples to increase batch diversity. Such approaches are attractive at test time because they are training-free and operate by modifying the sampling vector field; however, their effectiveness depends on the representation on which similarity/diversity is measured. When similarity is computed in the noisy latent domain (the $x_t$ space) early timesteps suffer from low signal-to-noise ratio and weak semantic structure, reducing the impact of repulsion; this motivates strategies that measure repulsion on cleaner estimates of the sample (e.g., analytically estimated $\hat x_0$) to obtain semantically meaningful forces \citep{corso2023particle, rombach2022high}.

\paragraph{Reward-based diversity and RL fine-tuning.}
An alternative is to bake diversity into the training objective, e.g., via diversity-promoting rewards or entropy-regularized RL. Such methods can achieve strong improvements but require costly fine-tuning and careful reward design to avoid gaming; they also illustrate the theoretical result that naive reward maximization tends to collapse the output distribution unless regularized or explicitly diversified \citep{miao2024training, jena2025elucidating, black2024training}.

\paragraph{Latent-space regularization and multi-policy blending.}
Some works propose regularizing internal representations or maintaining ensembles/mixes of base and fine-tuned policies to preserve structural variety while improving alignment. Annealed schedules and importance-guided blending (AIG-like strategies) gradually transition between base and adapted models; spatially- or entropy-adaptive blending refines this idea by letting the model choose when and where to apply stronger alignment pressure \citep{zhang2024annealed, podell2024sdxl}. Multi-policy mixtures can be implemented at training time (ensembles) or at inference time via model interpolation, each with different computational and stability trade-offs \citep{clark2024draft}.

\paragraph{Attention-based and structural steering.}
Manipulating cross- and self-attention maps has been shown effective for compositional control and for avoiding background/subject collapse: by inspecting attention entropy and token-specific maps one can identify semantically important regions and apply different alignment strengths regionally. Prompt-to-Prompt and related attention-control techniques provide practical tools for such interventions, while analytic studies of attention in Stable Diffusion expose mechanisms to extract reliable spatial masks for regional control \citep{hertz2023prompt, liu2024understanding}.

\paragraph{Trade-offs, limitations, and hybrid designs.}
Training-based diversity methods are powerful but costly; pure test-time methods are efficient but sometimes brittle or myopic. Hybrid ideas—e.g., using test-time repulsion measured on clean-image estimates combined with region-aware model blending and harmonization—seek the best of both worlds. Our proposed Clean-Manifold Diversity Guidance and Object-Aware Entropy Guidance fit precisely into this hybrid category: they are test-time, training-free, but operate on semantically meaningful estimates (the $\hat x_0$ manifold and attention-derived masks) to produce robust diversity gains without fine-tuning \citep{corso2023particle, miao2024training, hertz2023prompt}.

\subsection{Evaluation Metrics for Alignment and Diversity}
Measuring alignment and diversity requires multiple complementary metrics; no single score suffices.

\paragraph{Quality / alignment metrics.}
Commonly used quality and alignment indicators include CLIP-based similarity between prompt and image (``CLIP score''), human preference rates (A/B comparisons), R-Precision (retrieval-based alignment), and learned preference predictors such as PickScore. Each has strengths and biases: CLIP is cheap and correlates with semantic alignment but is sensitive to style and dataset biases; learned scorers can be more faithful to human judgements but risk overfitting to annotator idiosyncrasies \citep{radford2021learning, kirstain2023pick}.

\paragraph{Diversity metrics.}
Diversity is measured at batch and population levels. Intra-batch perceptual metrics such as LPIPS quantify perceptual distance between samples in the same prompt-conditioned batch; coverage and precision/recall for generative models estimate how well the model spans the target distribution. Entropy-based measures (e.g., the conditional output entropy estimated via sampling or model logits) offer another view but are harder to estimate for diffusion models. Visual diversity should be reported both as aggregated statistics (mean intra-batch LPIPS, variance of feature descriptors) and as distributions (histograms, CDFs) to reveal tail behaviour \citep{zhang2018unreasonable, heusel2017gans}.

\paragraph{Composite evaluations and Pareto analysis.}
Because alignment and diversity trade off, the recommended evaluation protocol plots a \emph{Diversity vs.\ Quality} Pareto curve (e.g., intra-batch LPIPS on the x-axis vs.\ CLIP score or PickScore on the y-axis). This reveals how methods shift the trade-off frontier. Complement these quantitative curves with targeted human evaluation on a representative prompt set and with qualitative inspection (example grids showing where diversity increases but alignment remains acceptable) \citep{jena2025elucidating, miao2024training}.

\paragraph{Robustness and statistical testing.}
Report metrics over a diverse prompt suite (multiple seeds, prompt types, subject complexity) and include statistical significance tests when comparing methods. Also evaluate failure modes: background collapse, subject identity drift, and visual artifacts introduced by blending/harmonization \citep{li2024alignment}.

\subsection{Relation of Prior Work to Our Proposal}
Prior test-time repulsive methods (e.g., particle-based guidance) motivate batch-level steering but are limited when similarity is measured in noisy latent space \citep{corso2023particle}. Reward-diversity RL approaches demonstrate the potential but are computationally expensive and prone to reward over-optimization \citep{miao2024training, black2024training}. Attention-based interventions show the utility of internal signals for regional control \citep{hertz2023prompt, liu2024understanding}. 

Our framework combines these insights: (1) compute repulsive/diversity forces on analytic clean estimates ($\hat x_0$) to obtain semantically meaningful batch repulsion early in sampling, and (2) use attention-entropy-driven, region-aware blending between base and fine-tuned models with a frequency harmonization step to preserve global lighting while keeping structural diversity. This combination is designed to expand the Pareto frontier between diversity (LPIPS) and alignment (CLIP/PickScore) more effectively than methods that operate solely in $x_t$ or rely on fixed schedules \citep{corso2023particle, jena2025elucidating, hertz2023prompt}.

\subsection{Limitations of Existing Approaches and Research Gaps}

Despite the significant progress in alignment and diversity control for diffusion models, several important limitations remain insufficiently addressed in the literature.

\paragraph{Noisy latent space versus semantic manifold.}
Many test-time diversity methods compute similarity or repulsion directly in the intermediate noisy latent space ($x_t$). However, early denoising steps are dominated by noise with low semantic signal-to-noise ratio, meaning that distances measured in this space do not reliably correspond to perceptual or semantic similarity. As a result, diversity forces may be weak or misaligned during the most influential stages of sampling. Prior work has largely overlooked the potential benefit of computing diversity constraints on cleaner estimates of the sample (e.g., analytically predicted $\hat{x}_0$), leaving a gap between theoretical diversity objectives and their practical effectiveness \citep{corso2023particle, rombach2022high}.

\paragraph{Lack of region-aware adaptive alignment.}
Existing alignment methods typically apply a global guidance strength across the entire image. However, semantic importance is spatially heterogeneous: foreground objects often require stronger alignment, while backgrounds benefit from weaker constraints to preserve diversity. Although attention maps have been studied for interpretability and editing control, they are rarely used to drive adaptive alignment schedules that vary across spatial regions and timesteps \citep{liu2024understanding, hertz2023prompt}.

\paragraph{Model blending without consistency preservation.}
Recent approaches that combine base and fine-tuned models through interpolation or scheduling improve robustness but often introduce visual artifacts such as lighting inconsistency or high-frequency noise mismatch. The literature provides limited discussion on frequency-domain consistency or harmonization mechanisms after blending multiple model predictions \citep{clark2024draft, zhang2024annealed}. Addressing this issue is important for maintaining perceptual realism while enabling flexible alignment control.

\paragraph{Implications for our approach.}
These limitations motivate our proposed framework. By computing diversity forces on clean-manifold estimates, introducing attention-driven region-aware blending between models, and applying frequency-aware harmonization, our method aims to overcome the weaknesses of prior test-time alignment techniques while remaining training-free and computationally practical.

% =============================================================================
\section{Proposed Methods}
\label{sec:methods}
% =============================================================================

\subsection{Method 1: Early-Step Diversity Guidance}
\label{sec:cmdg}

Existing test-time diversity methods, such as particle-based guidance~\citep{corso2023particle}, introduce repulsive forces between concurrently sampled images to prevent mode collapse within a batch. We build on this idea and propose \textbf{Early-Step Diversity Guidance}, a training-free inference-time framework that computes a structural similarity loss over a batch of latent samples and applies the resulting gradient to steer their sampling trajectories apart during the critical early denoising steps. We investigate two formulations that differ in the domain on which diversity is measured: (i)~\textit{Latent-Space Diversity Guidance}, which operates directly on the noisy latent $\mathbf{x}_t$, and (ii)~\textit{Clean-Manifold Diversity Guidance (CMDG)}, which first projects $\mathbf{x}_t$ onto the clean image manifold via Tweedie's formula. The framework operates in three phases: Diversity Domain Selection, Structural Diversity Computation, and Early-Step Gradient Steering.

\subsubsection{Phase 1: Diversity Domain Selection}

At each denoising timestep $t$, the model predicts the noise component $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, c)$. The diversity loss can be computed on two domains:

\textbf{Latent-space domain (primary).} The diversity loss is computed directly on the current noisy latent $\mathbf{x}_t$. This formulation is simple and efficient: it requires no additional forward computation beyond the standard denoising pass. Despite operating in the noisy domain, the structural pooling in Phase~2 mitigates high-frequency noise, allowing the gradient to capture meaningful layout-level differences even at moderate noise levels.

\textbf{Clean-manifold domain (variant).} Alternatively, we recover an analytical estimate of the clean image using Tweedie's formula~\citep{robbins1956empirical}:
\begin{equation}
    \hat{\mathbf{x}}_0 = \frac{\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\, \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, c)}{\sqrt{\bar{\alpha}_t}},
\end{equation}
where $\bar{\alpha}_t = \prod_{s=1}^{t} (1 - \alpha_s)$ is the cumulative noise schedule coefficient. This estimate preserves the global semantic layout of the emerging image. We clamp $\hat{\mathbf{x}}_0$ to the range $[-20, 20]$ to prevent numerical instability. While this projection provides a semantically richer signal, it introduces additional computation and, as shown in Section~\ref{sec:experiments}, can over-steer samples at the cost of alignment and image quality.

\subsubsection{Phase 2: Structural Diversity Loss}

Given a batch of $B$ representations $\{\mathbf{z}^{(i)}\}_{i=1}^{B}$ (where $\mathbf{z}$ is either $\mathbf{x}_t$ or $\hat{\mathbf{x}}_0$ depending on the chosen domain), we define a pairwise similarity loss. To focus on global composition rather than fine texture, we first apply spatial average pooling to reduce each representation to an $8 \times 8$ spatial resolution, then flatten and compute cosine similarity:
\begin{equation}
    \mathbf{f}^{(i)} = \text{flatten}\!\left(\text{AvgPool}_{8 \times 8}(\mathbf{z}^{(i)})\right), \quad \bar{\mathbf{f}}^{(i)} = \frac{\mathbf{f}^{(i)}}{\|\mathbf{f}^{(i)}\|_2}
\end{equation}
\begin{equation}
    \mathcal{L}_{\text{div}} = \frac{1}{B(B-1)} \sum_{i \neq j} \bar{\mathbf{f}}^{(i)} \cdot \bar{\mathbf{f}}^{(j)}
\end{equation}
This loss is minimized when batch members have maximally dissimilar global layouts. The average pooling to $8 \times 8$ is critical: it abstracts away pixel-level noise while preserving compositional structure (e.g., viewpoint, object placement, scene layout).

We compute the gradient of this loss with respect to the input representation:
\begin{equation}
    \mathbf{g} = \nabla_{\mathbf{z}} \mathcal{L}_{\text{div}},
\end{equation}
and normalize it to unit batch norm ($\|\mathbf{g}\|_{\text{batch}} = 1$) so that the diversity strength is controlled solely by the hyperparameter $\lambda_{\text{div}}$.

\subsubsection{Phase 3: Early-Step Gradient Steering}

The diversity gradient is applied as a corrective perturbation \textit{after} the standard scheduler step. At each guided timestep, the denoising update proceeds normally via classifier-free guidance (Eq.~\ref{eq:cfg}), producing the next latent $\mathbf{x}_{t-1}$. We then subtract the scaled diversity gradient:
\begin{equation}
    \mathbf{x}_{t-1} \leftarrow \mathbf{x}_{t-1} - \lambda_{\text{div}} \cdot w_t \cdot \mathbf{g},
\end{equation}
where $\lambda_{\text{div}}$ is the diversity scale and $w_t$ is a temporal weight that modulates the guidance strength across steps.

Crucially, diversity guidance is only applied during the first $k$ fraction of denoising steps (controlled by the \textit{early-stop ratio}), after which standard denoising proceeds unmodified. This design is motivated by the observation that global image structure is determined during the earliest steps, while later steps refine local textures and details. Applying diversity forces beyond this critical window wastes computation and risks degrading fine-grained quality without further diversity benefit.

The temporal weight $w_t$ supports several scheduling strategies: \textit{constant} ($w_t = 1$), \textit{linear decay} ($w_t = 1 - t/k$), and \textit{cosine decay} ($w_t = \frac{1}{2}(1 + \cos(\pi t / k))$). In our experiments, we find that constant weighting with $k = 0.3$ (i.e., guiding only the first 30\% of steps) provides the best balance between diversity improvement and alignment preservation. As shown in Section~\ref{sec:experiments}, the latent-space variant achieves the most favorable diversity--alignment tradeoff, significantly improving diversity while preserving the Fine-Tuned model's image quality and alignment scores. The clean-manifold variant achieves higher raw diversity but at a greater cost to alignment and perceptual quality.

\subsection{Method 2: Object-Aware Entropy Guidance (OAEG)}
\label{sec:oaeg}

While early-step trajectory interventions can ensure batch diversity, they do not resolve the intra-image conflict where highly-aligned fine-tuned models suffer from severe mode collapse, generating identical background environments across varying seeds. To combat this, we propose \textbf{Object-Aware Entropy Guidance (OAEG)}, a training-free inference framework that intelligently blends predictions from a highly diverse \textit{Base Model} ($\theta_{\text{base}}$) and a highly-aligned \textit{Fine-Tuned Model} ($\theta_{\text{ft}}$). 

The core intuition is that complex background structures should be governed by the Base model utilizing a low CFG scale, preserving spatial entropy, while specific semantic subjects should be rendered by the Fine-Tuned model at a high CFG scale to ensure aesthetic alignment. OAEG achieves this dynamic blending via a three-phase procedure during the reverse diffusion process: Dynamic Entropy Sensing, Multi-Subject Semantic Masking, and Regional Composition.

\subsubsection{Phase 1: Dynamic Entropy-Based Temporal Sensing}
The structural layout of a generated image generally solidifies during the earliest timesteps. We mathematically sense this commitment by measuring the Shannon Entropy of the cross-attention maps extracted from the Base U-Net. For a batch of cross-attention matrices $\mathbf{A}$, where $\mathbf{A}_{i,k}$ represents the attention probability of spatial patch $i$ to textual token $k$, the global entropy is computed over the conditional pass:
\begin{equation}
    E_t = - \frac{1}{H W} \sum_{i,j} \sum_k \mathbf{A}_{i,j,k} \log(\mathbf{A}_{i,j,k} + \epsilon)
\end{equation}
Because the maximum possible entropy shifts significantly depending on the prompt's token length and structural complexity, a static threshold is insufficient. Instead, we dynamically calibrate our threshold at the start of inference. We record the initial entropy $E_0$ at step $t=0$, and define our transition threshold as $\tau_{\text{entropy}} = E_0 / 2.0$.

We then define an adaptive annealing factor $\gamma \in [0, 1]$ using a sigmoid scheduling function governed by slope $s$:
\begin{equation}
    \gamma = \sigma \left( \frac{E_t - \tau_{\text{entropy}}}{s} \right)
\end{equation}
When the prompt layout is unresolved (high entropy), $\gamma \to 1$, allowing the Base model to establish a diverse structural foundation. As the layout locks (low entropy), $\gamma \to 0$, initiating a smooth transition toward the Fine-Tuned model's textural refinement.

\subsubsection{Phase 2: Multi-Subject Semantic Masking}

To prevent the Fine-Tuned model from inducing background collapse once $\gamma$ decays, we strictly confine its influence to the spatial regions occupied by the target subjects. We achieve this by exploiting the cross-attention mechanisms inherently present within the Base U-Net. 

During the reverse diffusion process, spatial features (projected as Queries, $\mathbf{Q} \in \mathbb{R}^{(H \times W) \times d}$) interact with the textual prompt embeddings (projected as Keys, $\mathbf{K} \in \mathbb{R}^{L \times d}$, where $L$ is the sequence length). The resulting cross-attention matrix $\mathbf{A} = \text{Softmax}(\mathbf{Q}\mathbf{K}^T / \sqrt{d})$ quantifies the semantic affinity between every spatial patch $(i,j)$ and every text token $k$. We specifically extract attention maps from the layers operating at the $16 \times 16$ spatial resolution, which empirically offer the optimal balance between high-level semantic understanding and spatial localization.

Given a set of target subject tokens $\mathcal{S} = \{k_1, k_2, \dots\}$ (e.g., indices for "lion" and "snake"), we isolate their corresponding attention probability maps. To robustly localize multiple distinct subjects within a single prompt, we aggregate these maps using a pixel-wise maximum operation, effectively functioning as a continuous logical OR. The raw multi-subject mask is defined as:
\begin{equation}
    M_{\text{raw}}^{(i,j)} = \max_{k \in \mathcal{S}} \, \mathbf{A}_{i,j,k}
\end{equation}

Because cross-attention distributions are inherently cloudy and contextually diffuse, utilizing $M_{\text{raw}}$ directly would cause the Fine-Tuned model's outputs to ``leak'' into the background. To enforce a strict semantic boundary, we apply a sequence of deterministic transformations. First, we upsample $M_{\text{raw}}$ to the target latent resolution ($64 \times 64$) via bilinear interpolation. Next, we apply instance-wise Min-Max normalization to stretch the attention values to a standardized $[0, 1]$ range:
\begin{equation}
    M_{\text{norm}} = \frac{M_{\text{raw}} - \min(M_{\text{raw}})}{\max(M_{\text{raw}}) - \min(M_{\text{raw}}) + \epsilon}
\end{equation}

Finally, we apply a strict threshold $\tau_{\text{mask}}$ to eliminate weak contextual bleed, yielding the final binarized spatial mask:
\begin{equation}
    M_{\text{soft}}^{(i,j)} = \mathbb{I}\left(M_{\text{norm}}^{(i,j)} > \tau_{\text{mask}}\right)
\end{equation}
where $\mathbb{I}(\cdot)$ is the indicator function. Notably, we intentionally omit any subsequent spatial filtering (such as Gaussian blurring) on the resulting mask. Our experiments demonstrated that blurring artificially expands the mask's footprint, inadvertently allowing the Fine-Tuned model to regularize and homogenize the immediate surrounding environment. By maintaining a sharp, thresholded boundary, we ensure maximum structural diversity in the background while perfectly localizing the high-fidelity aesthetic polish to the subjects.

\subsubsection{Phase 3: Regional Composition}
With our temporal schedule $\gamma$ and spatial mask $M_{\text{soft}}$ computed at timestep $t$, we blend the noise predictions $\epsilon_{\text{base}}$ and $\epsilon_{\text{ft}}$. The subject pixels transition aggressively toward the Fine-Tuned model, while the background pixels are constrained to preserve Base model diversity. 

We introduce a background damping factor $d \ll 1$ (e.g., $d=0.05$), ensuring the Fine-Tuned model barely influences non-subject regions:
\begin{align}
    \epsilon_{\text{subj}} &= (1 - \gamma)\epsilon_{\text{ft}} + \gamma\epsilon_{\text{base}} \\
    \epsilon_{\text{bg}} &= (1 - d \cdot \gamma)\epsilon_{\text{base}} + (d \cdot \gamma)\epsilon_{\text{ft}} 
\end{align}
The final composite noise prediction passed to the scheduler is:
\begin{equation}
    \epsilon_{\text{final}} = M_{\text{soft}} \odot \epsilon_{\text{subj}} + (1 - M_{\text{soft}}) \odot \epsilon_{\text{bg}}
\end{equation}

\textbf{A Note on Global Frequency Harmonization:} In our initial theoretical design, we hypothesized that applying a Gaussian Low-Pass Filter to harmonize global lighting from $\epsilon_{\text{ft}}$ with high-frequency structures from $\epsilon_{\text{final}}$ would improve visual coherence. However, extensive empirical testing revealed that overriding the background's low frequencies with the Fine-Tuned model's outputs drastically homogenized the environmental lighting, inadvertently restoring partial mode collapse. We found that directly passing our semantically masked composite noise $\epsilon_{\text{final}}$ strictly preserved the background variance while maintaining seamless subject integration. Thus, the explicit frequency filtering step was discarded in our final algorithm in favor of pure masked latent composition.


% TODO Placeholder for Method 3

% =============================================================================
\section{Experiments and Results}
\label{sec:experiments}
% =============================================================================

\subsection{Shared Experimental Setup}
\label{sec:experimental_setup}

Both methods are evaluated using a common set of models, sampling configurations, and metrics to enable direct comparison.

\textbf{Models:} We utilize Stable Diffusion v1.5 (\texttt{runwayml/stable-diffusion-v1-5}) as our highly-diverse, low-alignment \textit{Base Model}, and DreamShaper-8 (\texttt{Lykon/dreamshaper-8}) as our highly-aligned, low-diversity \textit{Fine-Tuned Model}. All image generations are executed using the DDIM sampling algorithm to ensure deterministic latent trajectories.

\textbf{Common Hyperparameters:} All images across all methods are generated using $T=50$ denoising steps and strictly matching deterministic latent noise seeds. This isolates any observed structural differences purely to the model intervention rather than initial noise variance. Batches of $B=4$ images are generated per prompt.

\textbf{Metrics:} We evaluate the generated batches along two orthogonal axes. To quantitatively measure spatial and compositional variance, we compute intra-batch \textbf{LPIPS} utilizing an AlexNet backbone. For alignment, we report \textbf{CLIP Score} (cosine similarity between CLIP image and text embeddings), \textbf{ImageReward} (a learned human-preference predictor), and \textbf{PickScore} (a ViT-H-based preference evaluator fine-tuned on Pick-a-Pic).

% -----------------------------------------------------------------------------
\subsection{Experiments for Method 1: CMDG}
\label{sec:exp_cmdg}
% -----------------------------------------------------------------------------

\subsubsection{Setup}

CMDG is evaluated on the Fine-Tuned model (DreamShaper-8) with CFG scale $w=7.5$. We compare four configurations: (i)~\textit{Base Model baseline} (SD v1.5, $w=3.5$, no guidance), representing the diversity upper bound; (ii)~\textit{Fine-Tuned baseline} (DreamShaper-8, $w=7.5$, no guidance), representing the alignment upper bound; (iii)~\textit{Naive latent-space guidance (ours)}, which computes the diversity gradient directly on $\mathbf{x}_t$ with $\lambda_{\text{div}} = 10.0$; and (iv)~\textit{CMDG (ours)}, which computes the diversity gradient on the clean estimate $\hat{\mathbf{x}}_0$ with $\lambda_{\text{div}} = 5.0$. Both guidance variants share an early-stop ratio $k = 0.3$, loss type \textit{structural} (average-pooled cosine similarity), and weight schedule \textit{constant}.

Prompts are drawn from a suite of 43 prompts spanning simple single-object scenes, compositional multi-object layouts, and stylistic descriptions.

\subsubsection{Quantitative Results}

Table~\ref{tab:cmdg_metrics} presents the average metrics across all 43 evaluation prompts for each configuration. The Fine-Tuned baseline (DreamShaper-8) achieves the highest alignment scores (ImageReward: $1.022$) but exhibits the lowest diversity (LPIPS: $0.563$), confirming the alignment--diversity tradeoff. The Base Model baseline (SD v1.5) provides the reference diversity level (LPIPS: $0.675$) at lower alignment (ImageReward: $0.431$).

Our \textbf{Latent-Space Diversity Guidance} achieves the most favorable tradeoff among all configurations. It increases LPIPS from $0.563$ to $0.653$---recovering $80\%$ of the diversity gap to the Base Model---while \textit{fully preserving} alignment: it achieves the highest CLIP Score of all four methods ($0.322$) and retains $83\%$ of the Fine-Tuned model's ImageReward ($0.851$ vs.\ $1.022$). This demonstrates that direct latent-domain repulsion, when restricted to the early denoising steps and combined with structural pooling, provides a highly effective and efficient diversity intervention without meaningfully compromising image quality.

The clean-manifold variant (CMDG) achieves higher raw diversity (LPIPS: $0.753$), surpassing even the Base Model. However, this comes at a substantially larger cost to perceptual quality and alignment (ImageReward: $0.665$, a $35\%$ drop from the Fine-Tuned baseline), indicating that the stronger Tweedie-based gradients tend to over-steer the sampling trajectories. The latent-space variant thus occupies a more practical operating point on the Pareto frontier, offering a strong diversity boost with minimal quality degradation.

\begin{table}[h]
\centering
\caption{Quantitative comparison for Early-Step Diversity Guidance, averaged across 43 evaluation prompts. The latent-space variant achieves the best diversity--alignment balance, while the clean-manifold variant (CMDG) trades alignment for higher raw diversity.}
\label{tab:cmdg_metrics}
\begin{tabular}{l c c c c}
\toprule
\textbf{Method} & \textbf{LPIPS $\uparrow$} & \textbf{CLIP Score $\uparrow$} & \textbf{CLIP Div.\ $\uparrow$} & \textbf{ImageReward $\uparrow$} \\
\midrule
Base Model (SD v1.5, $w\!=\!3.5$)       & 0.675 & 0.317 & 0.159 & 0.431 \\
Fine-Tuned (DreamShaper-8, $w\!=\!7.5$) & 0.563 & 0.321 & 0.089 & 1.022 \\
\textbf{Latent-Space Guidance (ours, $\lambda\!=\!10$)}  & \textbf{0.653} & \textbf{0.322} & 0.108 & \textbf{0.851} \\
CMDG (ours, $\lambda\!=\!5$)   & 0.753 & 0.318 & 0.136 & 0.665 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Qualitative Results}

We highlight qualitative comparisons on three representative prompts that span different complexity levels, using four fixed noise seeds per method.

\textbf{Single Subject:} For the prompt \textit{``A beautiful cinematic shark,''} the Fine-Tuned baseline produces near-identical shark poses and water environments across seeds (LPIPS: $0.575$). Our latent-space guidance substantially improves layout variation (LPIPS: $0.714$), generating different underwater scenes, camera angles, and shark poses while preserving the cinematic rendering quality of the Fine-Tuned model. CMDG pushes raw diversity further (LPIPS: $0.808$) but at noticeable cost to image coherence.

\textbf{Subject in Context:} For \textit{``A majestic knight standing in a mystical forest, cinematic lighting, highly detailed,''} the Fine-Tuned baseline freezes both the forest layout and the knight pose (LPIPS: $0.441$). Our latent-space guidance recovers substantial structural variation (LPIPS: $0.577$), producing diverse forest geometries and knight stances while maintaining the Fine-Tuned model's superior texture fidelity (CLIP Score: $0.360$ vs.\ $0.326$ for the Base Model). CMDG achieves higher diversity (LPIPS: $0.744$) but with diminished alignment.

\textbf{Complex Scene:} For \textit{``A beautiful cartoonlike village full of trees and houses,''} the Fine-Tuned model generates repetitive village layouts (LPIPS: $0.495$). Our latent-space guidance provides a meaningful improvement (LPIPS: $0.525$), introducing varied road configurations and building placements while retaining the vibrant cartoon-like aesthetic (ImageReward: $0.254$ vs.\ $0.770$ for the unguided Fine-Tuned baseline). CMDG achieves the highest raw diversity (LPIPS: $0.776$) but at a significant quality cost (ImageReward: $0.043$), confirming that the latent-space variant offers the more practical operating point.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{method1-shark.png}
    \caption{Visual comparison for \textit{``A beautiful cinematic shark''}. Our latent-space guidance generates diverse underwater environments and shark poses while preserving the Fine-Tuned model's cinematic quality.}
    \label{fig:cmdg_shark}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{method1-knight.png}
    \caption{Visual comparison for \textit{``A majestic knight standing in a mystical forest...''}. Our latent-space guidance recovers varied forest geometries and knight stances while retaining the Fine-Tuned model's texture fidelity.}
    \label{fig:cmdg_knight}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{method1-houses.png}
    \caption{Visual comparison for \textit{``A beautiful cartoonlike village full of trees and houses''}. Our latent-space guidance introduces diverse village topographies and road layouts while maintaining the cartoon-like aesthetic.}
    \label{fig:cmdg_houses}
\end{figure}

% -----------------------------------------------------------------------------
\subsection{Experiments for Method 2: OAEG}
\label{sec:exp_oaeg}
% -----------------------------------------------------------------------------

\subsubsection{Setup}

OAEG requires simultaneous inference from two models. The Base model operates at a low CFG scale ($w=3.5$) to encourage maximum structural diversity, while the Fine-Tuned model operates at a high CFG scale ($w=7.5$) to maximize aesthetic alignment. Inside the OAEG pipeline, these asymmetric CFG scales are evaluated simultaneously during the dual U-Net pass.

For the OAEG-specific hyperparameters, we set the attention mask threshold to $\tau_{\text{mask}} = 0.20$, the sigmoid slope to $s=0.2$ coupled with a dynamically calculated threshold ($\tau_{\text{entropy}} = E_0 / 2$), and a background damping factor of $d = 0.05$. This $5\%$ allowance prevents latent-space discontinuity at the mask boundaries while completely neutralizing the Fine-Tuned model's tendency to homogeneously collapse the background environment.

Evaluation prompts range from single-subject portraits (e.g., \textit{``A beautiful cinematic shark''}) to heavily contextualized figures (e.g., \textit{``A majestic knight standing in a mystical forest, cinematic lighting, highly detailed''}) and complex multi-subject interactions (e.g., \textit{``A beautiful cartoonlike village full of trees and houses''}).

\subsubsection{Quantitative Results}

Table~\ref{tab:oaeg_metrics} presents the objective evaluation of OAEG against the isolated Base and Fine-Tuned models. The empirical data encapsulates the fundamental tension: the Base model exhibits excellent structural diversity (LPIPS: $0.635$) but suffers from lower alignment (PickScore: $20.21$). Conversely, the Fine-Tuned model achieves superior human-preference validation (PickScore: $21.36$) but exhibits spatial mode collapse, reducing diversity (LPIPS: $0.5468$).

By intelligently routing the generation domains via dynamic entropy sensing and semantic masking, OAEG successfully bridges this gap. Our method restores near-baseline levels of structural diversity (LPIPS: $0.6271$) while preserving the aesthetic reward scaling of the preference-aligned checkpoint (PickScore: $20.843$).

\begin{table}[h]
\centering
\caption{Quantitative comparison of Diversity (LPIPS, $\uparrow$) and Alignment (PickScore, $\uparrow$) for OAEG, averaged across several testing prompts.}
\label{tab:oaeg_metrics}
\begin{tabular}{l c c}
\toprule
\textbf{Method} & \textbf{Diversity (LPIPS $\uparrow$)} & \textbf{Alignment (PickScore $\uparrow$)} \\
\midrule
Base Model (SD v1.5)       & 0.635 & 20.21 \\
Fine-Tuned (DreamShaper-8) & 0.5468 & 21.36 \\
\textbf{OAEG (Ours)}       & \textbf{0.6271} & \textbf{20.843} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Qualitative Results}

The quantitative success of OAEG is visually corroborated across a variety of prompt complexities. We highlight the generative outputs across four fixed noise seeds for three distinct test cases.

\textbf{Single Subject in Fluid Environments:} Figure~\ref{fig:oaeg_shark} evaluates the prompt \textit{``A beautiful cinematic shark.''} The Fine-Tuned model exhibits aggressive mode collapse, rendering an identical great white shark profile, water hue, and lighting angle across all four seeds. In contrast, OAEG leverages the Base model to generate vastly different underwater environments, camera angles, and shark species/poses, while the shark's skin texture and volumetric lighting are rendered with the Fine-Tuned model's cinematic fidelity.

\textbf{Subject-Environment Integration:} Figure~\ref{fig:oaeg_knight} evaluates \textit{``A majestic knight standing in a mystical forest, cinematic lighting, highly detailed.''} Here, the tradeoff is highly visible: the Base model generates varied forest layouts (paths, dense trees, clearings) but yields muddy, unpolished armor. The Fine-Tuned model yields hyper-realistic, gleaming armor, but ``freezes'' the forest geometry into a single deterministic layout. OAEG effectively randomizes the forest geometry (black regions in the mask) while rendering the knight (white regions) with pristine, preference-aligned textures.

\textbf{Complex Spatially-Distributed Subjects:} Figure~\ref{fig:oaeg_village} demonstrates OAEG's robustness using a prompt with heavily distributed target entities: \textit{``A beautiful cartoonlike village full of trees and houses.''} This case highlights the efficacy of our masking operation on scattered, repetitive objects. The internal masks successfully identify and isolate the numerous scattered \textit{houses} across the complex landscape. Consequently, OAEG delivers diverse village topographies, varied tree placements, and unique road layouts governed by the Base model, completely avoiding the rigid, identical circular village layouts produced by the Fine-Tuned baseline. Simultaneously, it ensures the vibrant, cartoon-like aesthetic and polished rendering of the houses remain optimal.
\newpage
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{iclr2026/shark.png}
    \caption{Visual comparison for \textit{``A beautiful cinematic shark''}. OAEG successfully diversifies the ocean environment and camera angles (matching the Base model's structural entropy) while preserving the high-quality textures of the Fine-Tuned model. The bottom row displays the extracted semantic mask for the token \textit{shark}.}
    \label{fig:oaeg_shark}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{iclr2026/knight.png}
    \caption{Visual comparison for \textit{``A majestic knight standing in a mystical forest...''}. OAEG completely circumvents the Fine-Tuned model's tendency to identically replicate the background trees across varying seeds.}
    \label{fig:oaeg_knight}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{iclr2026/houses.png}
    \caption{Visual comparison for \textit{``A hilarious, cinematic and brutal fight between crocodile and large snake''}. The dynamic multi-subject mask accurately isolates both entities, allowing OAEG to render diverse combat poses and swamp environments that the Fine-Tuned model inherently suppresses.}
    \label{fig:oaeg_village}
\end{figure}
\section{Discussion}
\label{sec:discussion}

Our experiments provide clear empirical evidence that the alignment--diversity tradeoff is not merely a side effect of specific training procedures, but rather a structural phenomenon inherent to preference optimization in diffusion models. As alignment pressure increases---whether through fine-tuning, reward optimization, or stronger classifier-free guidance---the model increasingly concentrates probability mass on a narrow subset of visually preferred solutions, leading to spatial and compositional mode collapse. This observation is consistent with the theoretical formulation presented in Section~\ref{sec:problem}, where reward-weighted optimization naturally reduces output entropy.

The proposed Object-Aware Entropy Guidance (OAEG) framework demonstrates that it is possible to partially decouple alignment and diversity by exploiting internal signals of the diffusion process rather than modifying model weights. In particular, two design principles emerge as critical. First, early-stage structural entropy plays a central role in determining final diversity. By allowing the Base model to dominate during high-entropy phases, OAEG preserves global layout variability before transitioning toward alignment refinement. Second, spatially localized alignment control is essential. Constraining the influence of the Fine-Tuned model to semantically relevant regions prevents global background homogenization, which is a primary driver of diversity collapse in preference-aligned checkpoints.

An interesting finding of our study concerns the role of frequency consistency. While we initially hypothesized that global frequency harmonization between Base and Fine-Tuned predictions would improve perceptual coherence, empirical results revealed the opposite: imposing Fine-Tuned low-frequency structure on the background reintroduced mode collapse. This suggests that diversity preservation depends not only on spatial routing but also on maintaining independent global statistics across samples.

Despite its effectiveness, OAEG has several limitations. First, the method requires dual forward passes through two diffusion models, which increases computational cost relative to standard inference. Second, the quality of the semantic mask depends on cross-attention reliability; prompts with highly abstract or ambiguous tokens may yield less precise localization. Third, the current entropy sensing mechanism is heuristic and may not generalize optimally across architectures or sampling schedules. Finally, our evaluation focuses primarily on perceptual diversity and preference alignment; broader notions of diversity, such as semantic coverage across prompt variations, remain to be explored.

More broadly, our findings suggest that alignment and diversity should not be treated as opposing objectives but as jointly controllable properties of the generative process. Methods that exploit internal uncertainty signals, spatial decomposition, and adaptive blending may offer a promising direction toward expanding the Pareto frontier between fidelity and variability without requiring expensive retraining.


\section{Conclusion}
\label{sec:conclusion}

In this work, we investigated the fundamental alignment--diversity tradeoff in text-to-image diffusion models from both theoretical and empirical perspectives. We first formalized the tradeoff as a consequence of reward-driven distribution shaping, showing that alignment optimization inherently concentrates probability mass and reduces entropy. We then surveyed existing alignment paradigms and diversity-preserving strategies, highlighting the limitations of both training-based and test-time approaches.

To better understand and mitigate this tradeoff, we introduced Object-Aware Entropy Guidance (OAEG), a training-free inference framework that dynamically blends predictions from a diverse base model and a highly aligned fine-tuned model. By leveraging entropy-aware temporal scheduling and spatially localized semantic masking derived from cross-attention maps, OAEG preserves global structural diversity while maintaining high-quality subject rendering. Experimental results demonstrate that our approach significantly improves the alignment--diversity Pareto frontier, recovering much of the Base model's variability without sacrificing the Fine-Tuned model's aesthetic fidelity.

Our study highlights an important insight: diversity collapse in aligned diffusion models is often driven by global background regularization rather than subject rendering itself. Controlling where and when alignment pressure is applied can therefore provide a powerful mechanism for balancing fidelity and variability without retraining.

Future work may explore more principled entropy estimation techniques, improved semantic localization strategies, and extensions to multi-modal or video diffusion models. Ultimately, developing alignment methods that explicitly preserve generative diversity will be essential for building creative AI systems that remain both controllable and expressive.

\bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}

\appendix
\section{Appendix}
% TODO: Additional results, implementation details, etc.

\end{document}