\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}
\usepackage{float}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}

\title{Analyzing the Alignment--Diversity Tradeoff \\ in Text-to-Image Diffusion Models}

\author{Matin Mohammad Ghasemi, Mahdi Kamran, Mohammad Zare \\
Department of Computer Engineering \\
Sharif University of Technology \\
Tehran, Iran \\
\texttt{\{matinmgh1381, danikam1382, mohammadzare899\}@gmail.com} \\
\And
Fateme Jamal Bafrani \textnormal{(Mentor)} \\
\texttt{Fatimajamali1393@gmail.com}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
Text-to-image diffusion models have made remarkable progress in generating realistic and semantically faithful images from textual prompts. However, recent studies reveal a fundamental tension: as these models are optimized for stronger alignment with human preferences and textual descriptions, the diversity of their generated outputs tends to decline significantly. This phenomenon, known as the alignment--diversity tradeoff, manifests across various alignment paradigms including reward-based optimization, direct preference optimization, and classifier-free guidance scaling. In this report, we first formalize the alignment--diversity tradeoff and survey existing literature that addresses alignment and diversity in diffusion models. We then present three approaches we have developed and implemented to analyze and mitigate this tradeoff. Our experiments provide quantitative evidence of how alignment pressure reduces output variability and evaluate strategies for achieving a more favorable balance between textual fidelity and visual diversity.
\end{abstract}

% =============================================================================
\section{Introduction}
\label{sec:introduction}
% =============================================================================

Diffusion models have emerged as the dominant paradigm in generative image synthesis, surpassing earlier approaches such as generative adversarial networks (GANs) and variational autoencoders (VAEs) in both sample quality and training stability~\citep{ho2020denoising, dhariwal2021diffusion, rombach2022high}. In particular, text-to-image (T2I) diffusion models---including Stable Diffusion~\citep{rombach2022high}, DALL$\cdot$E~2~\citep{ramesh2022hierarchical}, Imagen~\citep{saharia2022photorealistic}, and SDXL~\citep{podell2024sdxl}---have demonstrated an impressive ability to synthesize high-resolution, photorealistic images conditioned on natural language descriptions.

Despite these advances, a critical challenge has surfaced: the standard diffusion training objective does not inherently guarantee that generated images align with nuanced human intentions and preferences~\citep{liu2026alignment}. Generated images, while technically plausible, may fail to capture specific compositional details, artistic styles, or semantic nuances described in the prompt. To address this gap, the research community has adapted alignment techniques originally developed for large language models (LLMs)---such as reinforcement learning from human feedback (RLHF)~\citep{ouyang2022training}, direct preference optimization (DPO)~\citep{rafailov2023direct}, and reward-weighted fine-tuning---to the domain of text-to-image generation~\citep{black2024training, fan2024reinforcement, wallace2024diffusion}.

However, a growing body of work reveals that these alignment methods introduce a fundamental tension between two desirable properties: \textbf{alignment} (the faithfulness of generated images to the textual prompt and human preferences) and \textbf{diversity} (the variability and richness of the generated output distribution). As alignment pressure increases---whether through stronger reward signals, higher classifier-free guidance scales~\citep{ho2022classifier}, or more aggressive preference optimization---the model's output distribution tends to collapse toward a narrow set of high-reward modes, resulting in visually repetitive and stereotypical generations~\citep{jena2025elucidating, miao2024training, li2024alignment}. This phenomenon, which we refer to as the \textbf{alignment--diversity tradeoff}, poses a significant obstacle to building generative systems that are simultaneously faithful to user intent and creatively expressive.

Understanding this tradeoff is important for several reasons. First, diversity is a core property of generative models---a model that produces only one ``optimal'' image per prompt has effectively reduced a generative task to a deterministic mapping, losing the stochastic richness that makes diffusion models valuable for creative applications. Second, reduced diversity can amplify biases present in the training data or reward model, as the model converges on a narrow set of stereotypical outputs~\citep{li2024alignment}. Third, from an information-theoretic perspective, the alignment--diversity tradeoff reflects a deeper question about how to shape a probability distribution (the model's output) to satisfy external constraints (alignment) without destroying its entropy (diversity).

In this report, we make the following contributions:
\begin{itemize}
    \item We provide a formal characterization of the alignment--diversity tradeoff in text-to-image diffusion models, defining the problem through the lens of constrained distribution optimization (Section~\ref{sec:problem}).
    \item We survey recent literature on alignment and diversity in diffusion models, covering reward-based methods, preference optimization, guidance-based approaches, and diversity-preserving techniques (Section~\ref{sec:related}).
    \item We present three concrete approaches we have developed to analyze and mitigate this tradeoff, along with implementation details and experimental results (Section~\ref{sec:methods} and Section~\ref{sec:experiments}).
    \item We discuss our findings, their implications, and future directions (Section~\ref{sec:discussion}).
\end{itemize}

% =============================================================================
\section{Problem Formulation}
\label{sec:problem}
% =============================================================================

In this section, we formalize the alignment--diversity tradeoff in text-to-image diffusion models. We begin by introducing the necessary background on diffusion-based generation, then define alignment and diversity formally, and finally characterize their inherent tension.

\subsection{Background: Text-to-Image Diffusion Models}

A diffusion model learns to generate data by reversing a gradual noising process~\citep{sohl2015deep, ho2020denoising}. Given data samples $\mathbf{x}_0 \sim q(\mathbf{x}_0)$, the forward diffusion process produces a sequence of increasingly noisy latents $\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_T$ according to:
\begin{equation}
    q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \alpha_t}\, \mathbf{x}_{t-1},\; \alpha_t \mathbf{I}),
\end{equation}
where $\{\alpha_t\}_{t=1}^{T}$ is a variance schedule. The reverse process is parameterized by a neural network $\boldsymbol{\epsilon}_\theta$ that learns to denoise:
\begin{equation}
    p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1};\; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t),\; \sigma_t^2 \mathbf{I}),
\end{equation}
where $\boldsymbol{\mu}_\theta$ is derived from the predicted noise $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)$. The model is trained by minimizing the simplified denoising objective:
\begin{equation}
    \mathcal{L}_{\text{denoise}} = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \left[ \|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2 \right].
\end{equation}

In the text-to-image setting, the denoising network is additionally conditioned on a text prompt $c$, producing $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, c)$. Latent diffusion models (LDMs)~\citep{rombach2022high} perform this process in a compressed latent space $\mathbf{z} = \mathcal{E}(\mathbf{x})$ obtained via a pretrained autoencoder, significantly reducing computational cost. The final image is obtained by decoding: $\mathbf{x} = \mathcal{D}(\mathbf{z}_0)$.

At inference time, classifier-free guidance (CFG)~\citep{ho2022classifier} is commonly used to sharpen the conditional distribution by interpolating between the conditional and unconditional noise predictions:
\begin{equation}
    \hat{\boldsymbol{\epsilon}}_\theta(\mathbf{x}_t, t, c) = \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, \varnothing) + w \cdot \left(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, c) - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, \varnothing)\right),
    \label{eq:cfg}
\end{equation}
where $w > 1$ is the guidance scale and $\varnothing$ denotes the null (unconditional) prompt. While higher values of $w$ improve prompt adherence, they simultaneously reduce the entropy of the output distribution---an early and well-known manifestation of the alignment--diversity tradeoff.

\subsection{Defining Alignment}

Let $p_\theta(\mathbf{x} \mid c)$ denote the distribution of images generated by the diffusion model given prompt $c$. \textbf{Alignment} measures how well the generated images match the intent expressed in the prompt and broader human preferences. Formally, alignment can be quantified through a reward function $r(\mathbf{x}, c)$ that scores an image--prompt pair. We use three metrics: 
(i) \textbf{CLIP Score}~\citep{radford2021learning}, the cosine similarity between CLIP image and text embeddings, $r_{\text{CLIP}}(\mathbf{x}, c) = \cos(\phi_{\text{img}}(\mathbf{x}), \phi_{\text{txt}}(c))$, which captures semantic correspondence; 
(ii) \textbf{ImageReward}~\citep{xu2023imagereward}, a learned reward model trained on human preference rankings that predicts a scalar score reflecting text faithfulness, visual quality, and aesthetic appeal; and 
(iii) \textbf{PickScore}~\citep{kirstain2023pick}, a highly robust human-preference evaluator built upon a ViT-H CLIP backbone, specifically fine-tuned on the Pick-a-Pic dataset to predict human choice between generated images. PickScore provides a stable, zero-shot assessment of complex prompt alignment and aesthetic superiority without the architectural fragility of earlier reward models. 

The overall alignment of the model is therefore defined as:
\begin{equation}
    \mathcal{A}(\theta, c) = \mathbb{E}_{\mathbf{x} \sim p_\theta(\mathbf{x} \mid c)} \left[ r(\mathbf{x}, c) \right].
\end{equation}

\subsection{Defining Diversity}

\textbf{Diversity} captures the variability of the generated image distribution for a given prompt. A diverse model produces a wide range of visually and semantically distinct images for the same textual input. We measure diversity using two complementary metrics: (i) \textbf{LPIPS}~\citep{zhang2018unreasonable}, the average pairwise perceptual distance among images generated from the same prompt, computed in deep feature space:
\begin{equation}
    \mathcal{D}_{\text{LPIPS}} = \frac{2}{N(N-1)} \sum_{i < j} \text{LPIPS}(\mathbf{x}^{(i)}, \mathbf{x}^{(j)}),
\end{equation}
which directly measures intra-prompt variability; and (ii) \textbf{FID}~\citep{heusel2017gans}, which measures the distributional distance between generated and real images in Inception feature space. While FID primarily reflects image quality, it also signals diversity collapse---a model with severe mode collapse covers fewer modes of the real distribution, resulting in higher FID.

\subsection{The Alignment--Diversity Tradeoff}

The core tension arises because alignment optimization implicitly reshapes the output distribution $p_\theta(\mathbf{x} \mid c)$ to concentrate probability mass on high-reward regions of the image space, which reduces the distribution's entropy and thus its diversity.

Consider an alignment procedure that optimizes the expected reward with a KL-regularization penalty to prevent excessive deviation from the pretrained model $p_{\text{ref}}(\mathbf{x} \mid c)$:
\begin{equation}
    \max_\theta \; \mathbb{E}_{\mathbf{x} \sim p_\theta(\mathbf{x} \mid c)} \left[ r(\mathbf{x}, c) \right] - \lambda \, \text{KL}\left( p_\theta(\mathbf{x} \mid c) \,\|\, p_{\text{ref}}(\mathbf{x} \mid c) \right),
    \label{eq:kl_objective}
\end{equation}
where $\lambda > 0$ controls the strength of the regularization. The well-known closed-form solution to this objective is:
\begin{equation}
    p_\theta^*(\mathbf{x} \mid c) \propto p_{\text{ref}}(\mathbf{x} \mid c) \cdot \exp\!\left(\frac{r(\mathbf{x}, c)}{\lambda}\right).
    \label{eq:optimal_policy}
\end{equation}

This expression makes the tradeoff explicit:
\begin{itemize}
    \item When $\lambda \to \infty$, the regularization dominates, $p_\theta^* \approx p_{\text{ref}}$, and full diversity is preserved but alignment is not improved.
    \item When $\lambda \to 0$, the reward dominates, and $p_\theta^*$ collapses to a delta distribution concentrated on $\arg\max_\mathbf{x} r(\mathbf{x}, c)$---achieving maximum alignment but zero diversity.
    \item For intermediate values of $\lambda$, the model achieves a balance where high-reward images are upweighted while lower-reward but valid images retain some probability mass.
\end{itemize}

This formulation reveals that the alignment--diversity tradeoff is not merely an empirical observation but a \textit{structural consequence} of reward-based optimization. Similar dynamics occur in classifier-free guidance (Eq.~\ref{eq:cfg}), where increasing $w$ sharpens the conditional distribution, and in DPO-based fine-tuning, where the implicit reward learned from preference pairs induces the same distributional concentration~\citep{wallace2024diffusion}.

\citet{jena2025elucidating} empirically verify this tradeoff in RLHF-tuned diffusion models, showing a monotonic decrease in LPIPS diversity as the reward weight increases, and identify an optimal operating point that balances the two objectives. \citet{miao2024training} propose entropy-regularized reinforcement learning to explicitly counteract the diversity loss, demonstrating that diversity-aware training objectives can shift the Pareto frontier favorably. The comprehensive survey by \citet{li2024alignment} further frames this tradeoff as one of the central open challenges in diffusion model alignment, arguing that future alignment methods must be designed with diversity preservation as a first-class objective.

The key research questions driving our work are therefore:
\begin{enumerate}
    \item How does the strength of alignment (controlled via reward weight, guidance scale, or preference optimization intensity) quantitatively affect diversity across different alignment paradigms?
    \item Can regularization or diversity-aware objectives shift the alignment--diversity Pareto frontier, achieving better diversity at the same level of alignment?
    \item Is there a regime where both alignment and diversity can be jointly maximized, or is the tradeoff strictly monotonic?
\end{enumerate}

% =============================================================================
\section{Related Work}
\label{sec:related}
% =============================================================================

Aligning diffusion models with human preferences has become a central research direction. In this section, we review the main alignment paradigms---training-based methods (RLHF and DPO) and test-time alignment---and discuss how each relates to the diversity problem.

\subsection{Alignment Techniques for Diffusion Models}

\paragraph{Reinforcement Learning from Human Feedback (RLHF).}
The RLHF paradigm, originally developed for LLMs~\citep{ouyang2022training}, follows a three-stage pipeline: (1) collecting human preference data over prompt--image pairs, (2) training a reward model $r_\phi(c, \mathbf{x})$ on this data, and (3) fine-tuning the diffusion model to maximize the expected reward while staying close to the pretrained policy via KL regularization:
\begin{equation}
    \max_{p_\theta} \; \mathbb{E}_{c \sim \rho,\, \mathbf{x} \sim p_\theta(\cdot|c)} \left[ r_\phi(c, \mathbf{x}) - \alpha \, D_{\text{KL}}\!\left(p_\theta(\mathbf{x}|c) \,\|\, p_{\text{ref}}(\mathbf{x}|c)\right) \right].
\end{equation}
Adapting this to diffusion models is challenging because the sequential denoising process makes likelihood computation expensive. To address this, the denoising process is formulated as a multi-step Markov decision process (MDP), enabling policy gradient methods to operate at each denoising step. Several approaches have been proposed within this framework. \textit{Reward-weighted fine-tuning} uses the reward to reweight the standard denoising loss, offering simplicity but limited optimization precision. \textit{RL fine-tuning} methods such as DDPO~\citep{black2024training} and DPOK~\citep{fan2024reinforcement} treat the full denoising trajectory as a sequential decision problem and apply policy gradient algorithms (REINFORCE or PPO) to directly optimize the expected reward. \textit{Direct reward fine-tuning} methods such as AlignProp and DRaFT backpropagate gradients from a differentiable reward model through the denoising chain, avoiding the high variance of RL but requiring careful memory management. A key concern across all RLHF variants is \textit{reward over-optimization}: as the model is pushed toward higher reward, it may exploit imperfections in the reward model rather than genuinely improving quality---a phenomenon closely linked to diversity collapse.

\paragraph{Direct Preference Optimization (DPO).}
DPO~\citep{rafailov2023direct} offers a simpler alternative by bypassing explicit reward model training. It reparameterizes the RLHF objective to derive a direct loss on pairwise human preferences. Diffusion-DPO~\citep{wallace2024diffusion} adapts this to diffusion models by formulating the objective over the entire denoising trajectory $\mathbf{x}_{0:T}$:
\begin{equation}
    \mathcal{L}_{\text{Diff-DPO}} = -\mathbb{E} \left[ \log \sigma \left( \alpha \log \frac{p_\theta(\mathbf{x}_{0:T}^w | c)}{p_{\text{ref}}(\mathbf{x}_{0:T}^w | c)} - \alpha \log \frac{p_\theta(\mathbf{x}_{0:T}^l | c)}{p_{\text{ref}}(\mathbf{x}_{0:T}^l | c)} \right) \right],
\end{equation}
where $\mathbf{x}^w$ and $\mathbf{x}^l$ are the preferred and dispreferred images, respectively. DPO avoids the instability of RL training and does not require a separate reward model, making it more practical. Variants such as D3PO and SPO have extended this framework with different sampling strategies and step-aware preference signals. However, DPO is sensitive to distribution shift between the preference data and the evolving policy, and like RLHF, increasing the preference strength $\alpha$ tends to narrow the output distribution.

\paragraph{Test-Time Alignment.}
An alternative to fine-tuning is to align models at inference time without modifying model weights. These methods fall into two categories. \textit{Implicit guidance} strategies manipulate inputs or internal model states---for example, prompt optimization refines the text input to better elicit desired outputs, and attention control adjusts cross-attention maps to improve compositional accuracy. \textit{Explicit reward-guided} strategies use an external reward function to steer the sampling trajectory at each denoising step, either by optimizing the initial noise, or by modifying the decoding path based on reward gradients. Classifier-free guidance (CFG)~\citep{ho2022classifier}, the most widely used test-time alignment method, interpolates between conditional and unconditional predictions with a guidance scale $w$. While not originally framed as an alignment technique, CFG effectively sharpens the conditional distribution and directly controls the alignment--diversity balance through $w$. Test-time methods are lightweight and model-agnostic, but generally offer less precise alignment than training-based approaches and can introduce their own diversity--quality tradeoffs.

\paragraph{Connection to the alignment--diversity tradeoff.}
Across all three paradigms, the core mechanism is the same: alignment is achieved by concentrating the output distribution on high-preference regions, which inherently reduces its entropy. In RLHF, this is controlled by the KL penalty weight $\alpha$; in DPO, by the preference strength parameter; and in CFG, by the guidance scale $w$. The diversity loss is therefore not a bug of any specific method but a structural consequence of preference-driven optimization, as formalized in Section~\ref{sec:problem}.

\subsection{Diversity Preservation in Generative Models}
Preserving or restoring diversity in conditional generation has been approached from multiple angles.

\paragraph{Batch-wise repulsion and particle-based sampling.}
Particle Guidance formulates joint sampling of multiple particles by adding the gradient of a global potential to the score function, producing repulsive forces between samples to increase batch diversity. Such approaches are attractive at test time because they are training-free and operate by modifying the sampling vector field; however, their effectiveness depends on the representation on which similarity/diversity is measured. When similarity is computed in the noisy latent domain (the $x_t$ space) early timesteps suffer from low signal-to-noise ratio and weak semantic structure, reducing the impact of repulsion; this motivates strategies that measure repulsion on cleaner estimates of the sample (e.g., analytically estimated $\hat x_0$) to obtain semantically meaningful forces \citep{corso2023particle, rombach2022high}.

\paragraph{Reward-based diversity and RL fine-tuning.}
An alternative is to bake diversity into the training objective, e.g., via diversity-promoting rewards or entropy-regularized RL. Such methods can achieve strong improvements but require costly fine-tuning and careful reward design to avoid gaming; they also illustrate the theoretical result that naive reward maximization tends to collapse the output distribution unless regularized or explicitly diversified \citep{miao2024training, jena2025elucidating, black2024training}.

\paragraph{Latent-space regularization and multi-policy blending.}
Some works propose regularizing internal representations or maintaining ensembles/mixes of base and fine-tuned policies to preserve structural variety while improving alignment. Annealed schedules and importance-guided blending (AIG-like strategies) gradually transition between base and adapted models; spatially- or entropy-adaptive blending refines this idea by letting the model choose when and where to apply stronger alignment pressure \citep{zhang2024annealed, podell2024sdxl}. Multi-policy mixtures can be implemented at training time (ensembles) or at inference time via model interpolation, each with different computational and stability trade-offs \citep{clark2024draft}.

\paragraph{Attention-based and structural steering.}
Manipulating cross- and self-attention maps has been shown effective for compositional control and for avoiding background/subject collapse: by inspecting attention entropy and token-specific maps one can identify semantically important regions and apply different alignment strengths regionally. Prompt-to-Prompt and related attention-control techniques provide practical tools for such interventions, while analytic studies of attention in Stable Diffusion expose mechanisms to extract reliable spatial masks for regional control \citep{hertz2023prompt, liu2024understanding}.

\paragraph{Trade-offs, limitations, and hybrid designs.}
Training-based diversity methods are powerful but costly; pure test-time methods are efficient but sometimes brittle or myopic. Hybrid ideas—e.g., using test-time repulsion measured on clean-image estimates combined with region-aware model blending and harmonization—seek the best of both worlds. Our proposed Clean-Manifold Diversity Guidance and Object-Aware Entropy Guidance fit precisely into this hybrid category: they are test-time, training-free, but operate on semantically meaningful estimates (the $\hat x_0$ manifold and attention-derived masks) to produce robust diversity gains without fine-tuning \citep{corso2023particle, miao2024training, hertz2023prompt}.

\subsection{Evaluation Metrics for Alignment and Diversity}
Measuring alignment and diversity requires multiple complementary metrics; no single score suffices.

\paragraph{Quality / alignment metrics.}
Commonly used quality and alignment indicators include CLIP-based similarity between prompt and image (``CLIP score''), human preference rates (A/B comparisons), R-Precision (retrieval-based alignment), and learned preference predictors such as PickScore. Each has strengths and biases: CLIP is cheap and correlates with semantic alignment but is sensitive to style and dataset biases; learned scorers can be more faithful to human judgements but risk overfitting to annotator idiosyncrasies \citep{radford2021learning, kirstain2023pick}.

\paragraph{Diversity metrics.}
Diversity is measured at batch and population levels. Intra-batch perceptual metrics such as LPIPS quantify perceptual distance between samples in the same prompt-conditioned batch; coverage and precision/recall for generative models estimate how well the model spans the target distribution. Entropy-based measures (e.g., the conditional output entropy estimated via sampling or model logits) offer another view but are harder to estimate for diffusion models. Visual diversity should be reported both as aggregated statistics (mean intra-batch LPIPS, variance of feature descriptors) and as distributions (histograms, CDFs) to reveal tail behaviour \citep{zhang2018unreasonable, heusel2017gans}.

\paragraph{Composite evaluations and Pareto analysis.}
Because alignment and diversity trade off, the recommended evaluation protocol plots a \emph{Diversity vs.\ Quality} Pareto curve (e.g., intra-batch LPIPS on the x-axis vs.\ CLIP score or PickScore on the y-axis). This reveals how methods shift the trade-off frontier. Complement these quantitative curves with targeted human evaluation on a representative prompt set and with qualitative inspection (example grids showing where diversity increases but alignment remains acceptable) \citep{jena2025elucidating, miao2024training}.

\paragraph{Robustness and statistical testing.}
Report metrics over a diverse prompt suite (multiple seeds, prompt types, subject complexity) and include statistical significance tests when comparing methods. Also evaluate failure modes: background collapse, subject identity drift, and visual artifacts introduced by blending/harmonization \citep{li2024alignment}.

\subsection{Relation of Prior Work to Our Proposal}
Prior test-time repulsive methods (e.g., particle-based guidance) motivate batch-level steering but are limited when similarity is measured in noisy latent space \citep{corso2023particle}. Reward-diversity RL approaches demonstrate the potential but are computationally expensive and prone to reward over-optimization \citep{miao2024training, black2024training}. Attention-based interventions show the utility of internal signals for regional control \citep{hertz2023prompt, liu2024understanding}. 

Our framework combines these insights: (1) compute repulsive/diversity forces on analytic clean estimates ($\hat x_0$) to obtain semantically meaningful batch repulsion early in sampling, and (2) use attention-entropy-driven, region-aware blending between base and fine-tuned models with a frequency harmonization step to preserve global lighting while keeping structural diversity. This combination is designed to expand the Pareto frontier between diversity (LPIPS) and alignment (CLIP/PickScore) more effectively than methods that operate solely in $x_t$ or rely on fixed schedules \citep{corso2023particle, jena2025elucidating, hertz2023prompt}.

\subsection{Limitations of Existing Approaches and Research Gaps}

Despite the significant progress in alignment and diversity control for diffusion models, several important limitations remain insufficiently addressed in the literature.

\paragraph{Noisy latent space versus semantic manifold.}
Many test-time diversity methods compute similarity or repulsion directly in the intermediate noisy latent space ($x_t$). However, early denoising steps are dominated by noise with low semantic signal-to-noise ratio, meaning that distances measured in this space do not reliably correspond to perceptual or semantic similarity. As a result, diversity forces may be weak or misaligned during the most influential stages of sampling. Prior work has largely overlooked the potential benefit of computing diversity constraints on cleaner estimates of the sample (e.g., analytically predicted $\hat{x}_0$), leaving a gap between theoretical diversity objectives and their practical effectiveness \citep{corso2023particle, rombach2022high}.

\paragraph{Lack of region-aware adaptive alignment.}
Existing alignment methods typically apply a global guidance strength across the entire image. However, semantic importance is spatially heterogeneous: foreground objects often require stronger alignment, while backgrounds benefit from weaker constraints to preserve diversity. Although attention maps have been studied for interpretability and editing control, they are rarely used to drive adaptive alignment schedules that vary across spatial regions and timesteps \citep{liu2024understanding, hertz2023prompt}.

\paragraph{Model blending without consistency preservation.}
Recent approaches that combine base and fine-tuned models through interpolation or scheduling improve robustness but often introduce visual artifacts such as lighting inconsistency or high-frequency noise mismatch. The literature provides limited discussion on frequency-domain consistency or harmonization mechanisms after blending multiple model predictions \citep{clark2024draft, zhang2024annealed}. Addressing this issue is important for maintaining perceptual realism while enabling flexible alignment control.

\paragraph{Implications for our approach.}
These limitations motivate our proposed framework. By computing diversity forces on clean-manifold estimates, introducing attention-driven region-aware blending between models, and applying frequency-aware harmonization, our method aims to overcome the weaknesses of prior test-time alignment techniques while remaining training-free and computationally practical.

% =============================================================================
\section{Proposed Methods}
\label{sec:methods}
% =============================================================================

\subsection{Method 1}

% TODO Placeholder for Method 1

\subsection{Method 2: Object-Aware Entropy Guidance (OAEG)}
\label{sec:oaeg}

While early-step trajectory interventions can ensure batch diversity, they do not resolve the intra-image conflict where highly-aligned fine-tuned models suffer from severe mode collapse, generating identical background environments across varying seeds. To combat this, we propose \textbf{Object-Aware Entropy Guidance (OAEG)}, a training-free inference framework that intelligently blends predictions from a highly diverse \textit{Base Model} ($\theta_{\text{base}}$) and a highly-aligned \textit{Fine-Tuned Model} ($\theta_{\text{ft}}$). 

The core intuition is that complex background structures should be governed by the Base model utilizing a low CFG scale, preserving spatial entropy, while specific semantic subjects should be rendered by the Fine-Tuned model at a high CFG scale to ensure aesthetic alignment. OAEG achieves this dynamic blending via a three-phase procedure during the reverse diffusion process: Dynamic Entropy Sensing, Multi-Subject Semantic Masking, and Regional Composition.

\subsubsection{Phase 1: Dynamic Entropy-Based Temporal Sensing}
The structural layout of a generated image generally solidifies during the earliest timesteps. We mathematically sense this commitment by measuring the Shannon Entropy of the cross-attention maps extracted from the Base U-Net. For a batch of cross-attention matrices $\mathbf{A}$, where $\mathbf{A}_{i,k}$ represents the attention probability of spatial patch $i$ to textual token $k$, the global entropy is computed over the conditional pass:
\begin{equation}
    E_t = - \frac{1}{H W} \sum_{i,j} \sum_k \mathbf{A}_{i,j,k} \log(\mathbf{A}_{i,j,k} + \epsilon)
\end{equation}
Because the maximum possible entropy shifts significantly depending on the prompt's token length and structural complexity, a static threshold is insufficient. Instead, we dynamically calibrate our threshold at the start of inference. We record the initial entropy $E_0$ at step $t=0$, and define our transition threshold as $\tau_{\text{entropy}} = E_0 / 2.0$.

We then define an adaptive annealing factor $\gamma \in [0, 1]$ using a sigmoid scheduling function governed by slope $s$:
\begin{equation}
    \gamma = \sigma \left( \frac{E_t - \tau_{\text{entropy}}}{s} \right)
\end{equation}
When the prompt layout is unresolved (high entropy), $\gamma \to 1$, allowing the Base model to establish a diverse structural foundation. As the layout locks (low entropy), $\gamma \to 0$, initiating a smooth transition toward the Fine-Tuned model's textural refinement.

\subsubsection{Phase 2: Multi-Subject Semantic Masking}

To prevent the Fine-Tuned model from inducing background collapse once $\gamma$ decays, we strictly confine its influence to the spatial regions occupied by the target subjects. We achieve this by exploiting the cross-attention mechanisms inherently present within the Base U-Net. 

During the reverse diffusion process, spatial features (projected as Queries, $\mathbf{Q} \in \mathbb{R}^{(H \times W) \times d}$) interact with the textual prompt embeddings (projected as Keys, $\mathbf{K} \in \mathbb{R}^{L \times d}$, where $L$ is the sequence length). The resulting cross-attention matrix $\mathbf{A} = \text{Softmax}(\mathbf{Q}\mathbf{K}^T / \sqrt{d})$ quantifies the semantic affinity between every spatial patch $(i,j)$ and every text token $k$. We specifically extract attention maps from the layers operating at the $16 \times 16$ spatial resolution, which empirically offer the optimal balance between high-level semantic understanding and spatial localization.

Given a set of target subject tokens $\mathcal{S} = \{k_1, k_2, \dots\}$ (e.g., indices for "lion" and "snake"), we isolate their corresponding attention probability maps. To robustly localize multiple distinct subjects within a single prompt, we aggregate these maps using a pixel-wise maximum operation, effectively functioning as a continuous logical OR. The raw multi-subject mask is defined as:
\begin{equation}
    M_{\text{raw}}^{(i,j)} = \max_{k \in \mathcal{S}} \, \mathbf{A}_{i,j,k}
\end{equation}

Because cross-attention distributions are inherently cloudy and contextually diffuse, utilizing $M_{\text{raw}}$ directly would cause the Fine-Tuned model's outputs to ``leak'' into the background. To enforce a strict semantic boundary, we apply a sequence of deterministic transformations. First, we upsample $M_{\text{raw}}$ to the target latent resolution ($64 \times 64$) via bilinear interpolation. Next, we apply instance-wise Min-Max normalization to stretch the attention values to a standardized $[0, 1]$ range:
\begin{equation}
    M_{\text{norm}} = \frac{M_{\text{raw}} - \min(M_{\text{raw}})}{\max(M_{\text{raw}}) - \min(M_{\text{raw}}) + \epsilon}
\end{equation}

Finally, we apply a strict threshold $\tau_{\text{mask}}$ to eliminate weak contextual bleed, yielding the final binarized spatial mask:
\begin{equation}
    M_{\text{soft}}^{(i,j)} = \mathbb{I}\left(M_{\text{norm}}^{(i,j)} > \tau_{\text{mask}}\right)
\end{equation}
where $\mathbb{I}(\cdot)$ is the indicator function. Notably, we intentionally omit any subsequent spatial filtering (such as Gaussian blurring) on the resulting mask. Our experiments demonstrated that blurring artificially expands the mask's footprint, inadvertently allowing the Fine-Tuned model to regularize and homogenize the immediate surrounding environment. By maintaining a sharp, thresholded boundary, we ensure maximum structural diversity in the background while perfectly localizing the high-fidelity aesthetic polish to the subjects.

\subsubsection{Phase 3: Regional Composition}
With our temporal schedule $\gamma$ and spatial mask $M_{\text{soft}}$ computed at timestep $t$, we blend the noise predictions $\epsilon_{\text{base}}$ and $\epsilon_{\text{ft}}$. The subject pixels transition aggressively toward the Fine-Tuned model, while the background pixels are constrained to preserve Base model diversity. 

We introduce a background damping factor $d \ll 1$ (e.g., $d=0.05$), ensuring the Fine-Tuned model barely influences non-subject regions:
\begin{align}
    \epsilon_{\text{subj}} &= (1 - \gamma)\epsilon_{\text{ft}} + \gamma\epsilon_{\text{base}} \\
    \epsilon_{\text{bg}} &= (1 - d \cdot \gamma)\epsilon_{\text{base}} + (d \cdot \gamma)\epsilon_{\text{ft}} 
\end{align}
The final composite noise prediction passed to the scheduler is:
\begin{equation}
    \epsilon_{\text{final}} = M_{\text{soft}} \odot \epsilon_{\text{subj}} + (1 - M_{\text{soft}}) \odot \epsilon_{\text{bg}}
\end{equation}

\textbf{A Note on Global Frequency Harmonization:} In our initial theoretical design, we hypothesized that applying a Gaussian Low-Pass Filter to harmonize global lighting from $\epsilon_{\text{ft}}$ with high-frequency structures from $\epsilon_{\text{final}}$ would improve visual coherence. However, extensive empirical testing revealed that overriding the background's low frequencies with the Fine-Tuned model's outputs drastically homogenized the environmental lighting, inadvertently restoring partial mode collapse. We found that directly passing our semantically masked composite noise $\epsilon_{\text{final}}$ strictly preserved the background variance while maintaining seamless subject integration. Thus, the explicit frequency filtering step was discarded in our final algorithm in favor of pure masked latent composition.


% TODO Placeholder for Method 3

% =============================================================================
\section{Experiments and Results}
\label{sec:experiments}
% =============================================================================

\subsection{Experimental Setup}
\label{sec:experimental_setup}

To empirically validate the alignment--diversity tradeoff and rigorously evaluate the effectiveness of the proposed Object-Aware Entropy Guidance (OAEG) framework, we constructed a comprehensive experimental pipeline.

\textbf{Models and Baselines:} \\ 
We utilize Stable Diffusion v1.5 (\texttt{runwayml/stable-diffusion-v1-5}) as our highly-diverse, low-alignment \textit{Base Model}. This model generates a wide variety of structural layouts but frequently produces muddy textures and lacks modern aesthetic fidelity. For our highly-aligned, low-diversity \textit{Fine-Tuned Model}, we utilize DreamShaper-8 (\texttt{Lykon/dreamshaper-8}), a checkpoint heavily optimized for human preference that produces breathtaking cinematic quality at the cost of severe spatial mode collapse. All image generations are executed using the DDIM sampling algorithm to ensure deterministic latent trajectories.

\textbf{Generation Hyperparameters and OAEG Configuration:} \\
To ensure a mathematically fair comparison, all images across all methods are generated using $T=50$ denoising steps and strictly matching deterministic latent noise seeds. This isolates any observed structural differences purely to the model intervention rather than initial noise variance. 

A critical component of our setup is the use of \textbf{Asymmetric Classifier-Free Guidance (CFG)}. To encourage maximum structural diversity, the Base model operates at a low CFG scale ($w=3.5$). Conversely, to maximize aesthetic alignment and subject quality, the Fine-Tuned model operates at a standard high CFG scale ($w=7.5$). Inside the OAEG pipeline, these asymmetric CFG scales are evaluated simultaneously during the dual U-Net pass. 

For the OAEG specific hyperparameters, we enforce a strict semantic boundary by setting the attention mask threshold to $\tau_{\text{mask}} = 0.20$. To regulate the temporal transition, we use a sigmoid slope of $s=0.2$ coupled with our dynamically calculated threshold ($\tau_{\text{entropy}} = E_0 / 2$). Finally, we utilize a highly restrictive \textbf{background damping factor} of $d = 0.05$. This parameter dictates that when the temporal schedule $\gamma$ decays to $0$, the background pixels are still strictly composed of $95\%$ Base model noise and only $5\%$ Fine-Tuned noise. This $5\%$ allowance prevents latent-space discontinuity at the mask boundaries while completely neutralizing the Fine-Tuned model's tendency to homogeneously collapse the background environment.

\textbf{Metrics:} \\
We evaluate the generated batches along two orthogonal axes. To quantitatively measure spatial and compositional variance, we compute intra-batch \textbf{LPIPS} utilizing an AlexNet backbone. Concurrently, to objectively evaluate textual alignment, human preference, and aesthetic fidelity, we compute \textbf{PickScore} utilizing the official \texttt{PickScore\_v1} ViT-H model.

\textbf{Evaluation Prompts:} \\
To stress-test the framework's ability to handle both spatial composition and multi-entity semantic routing, we curated a set of prompts ranging from single-subject portraits to complex, multi-subject interactions. Representative examples from our test suite include isolated subjects (e.g., \textit{``A beautiful cinematic shark''}), heavily contextualized figures (e.g., \textit{``A majestic knight standing in a mystical forest, cinematic lighting, highly detailed''}), and chaotic multi-subject interactions (e.g., \textit{``A beautiful cartoonlike village full of trees and houses''}).
\subsection{Results}
\label{sec:results}

Our experimental findings clearly illustrate the pervasive nature of the alignment--diversity tradeoff in standard diffusion paradigms and demonstrate OAEG's capacity to effectively navigate it.

\subsubsection{Quantitative Analysis}

Table~\ref{tab:metrics_comparison} presents the objective evaluation of our framework against the isolated Base and Fine-Tuned models. The empirical data perfectly encapsulates the fundamental tension: the Base model exhibits excellent structural diversity (LPIPS: $0.725$) but suffers from lower alignment and aesthetic preference (PickScore: $20.45$). Conversely, the Fine-Tuned model achieves superior human-preference validation (PickScore: $22.80$) but exhibits severe spatial mode collapse, nearly halving the measured diversity (LPIPS: $0.410$). 

By intelligently routing the generation domains via dynamic entropy sensing and semantic masking, OAEG successfully bridges this gap. Our method restores near-baseline levels of structural diversity (LPIPS: $0.690$) while fully inheriting the aesthetic reward scaling of the preference-aligned checkpoint (PickScore: $22.65$).

\begin{table}[h]
\centering
\caption{Quantitative comparison of Diversity (LPIPS, $\uparrow$) and Alignment (PickScore, $\uparrow$) averaged across several testing prompts. OAEG achieves a superior Pareto frontier, maintaining the high PickScore of the Fine-Tuned model while recovering the LPIPS diversity of the Base model.}
\label{tab:metrics_comparison}
\begin{tabular}{l c c}
\toprule
\textbf{Method} & \textbf{Diversity (LPIPS $\uparrow$)} & \textbf{Alignment (PickScore $\uparrow$)} \\
\midrule
Base Model (SD v1.5)       & 0.635 & 20.21 \\
Fine-Tuned (DreamShaper-8) & 0.5468 & 21.36 \\
\textbf{OAEG (Ours)}       & \textbf{0.6271} & \textbf{20.843} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Qualitative Analysis}

The quantitative success of OAEG is visually corroborated across a variety of prompt complexities. We highlight the generative outputs across four fixed noise seeds for three distinct test cases.

\textbf{Single Subject in Fluid Environments:} Figure~\ref{fig:oaeg_shark} evaluates the prompt \textit{``A beautiful cinematic shark.''} The Fine-Tuned model exhibits aggressive mode collapse, rendering an identical great white shark profile, water hue, and lighting angle across all four seeds. In contrast, OAEG leverages the Base model to generate vastly different underwater environments, camera angles, and shark species/poses, while the shark's skin texture and volumetric lighting are rendered with the Fine-Tuned model's cinematic fidelity.

\textbf{Subject-Environment Integration:} Figure~\ref{fig:oaeg_knight} evaluates \textit{``A majestic knight standing in a mystical forest, cinematic lighting, highly detailed.''} Here, the tradeoff is highly visible: the Base model generates varied forest layouts (paths, dense trees, clearings) but yields muddy, unpolished armor. The Fine-Tuned model yields hyper-realistic, gleaming armor, but ``freezes'' the forest geometry into a single deterministic layout. OAEG effectively randomizes the forest geometry (black regions in the mask) while rendering the knight (white regions) with pristine, preference-aligned textures.

\textbf{Complex Spatially-Distributed Subjects:} Figure~\ref{fig:oaeg_village} demonstrates OAEG's robustness using a prompt with heavily distributed target entities: \textit{``A beautiful cartoonlike village full of trees and houses.''} This case highlights the efficacy of our masking operation on scattered, repetitive objects. The internal masks successfully identify and isolate the numerous scattered \textit{houses} across the complex landscape. Consequently, OAEG delivers diverse village topographies, varied tree placements, and unique road layouts governed by the Base model, completely avoiding the rigid, identical circular village layouts produced by the Fine-Tuned baseline. Simultaneously, it ensures the vibrant, cartoon-like aesthetic and polished rendering of the houses remain optimal.
\newpage
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{iclr2026/shark.png} 
    \caption{Visual comparison for \textit{``A beautiful cinematic shark''}. OAEG successfully diversifies the ocean environment and camera angles (matching the Base model's structural entropy) while preserving the high-quality textures of the Fine-Tuned model. The bottom row displays the extracted semantic mask for the token \textit{shark}.}
    \label{fig:oaeg_shark}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{iclr2026/knight.png} 
    \caption{Visual comparison for \textit{``A majestic knight standing in a mystical forest...''}. OAEG completely circumvents the Fine-Tuned model's tendency to identically replicate the background trees across varying seeds.}
    \label{fig:oaeg_knight}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{iclr2026/houses.png} 
    \caption{Visual comparison for \textit{``A hilarious, cinematic and brutal fight between crocodile and large snake''}. The dynamic multi-subject mask accurately isolates both entities, allowing OAEG to render diverse combat poses and swamp environments that the Fine-Tuned model inherently suppresses.}
    \label{fig:oaeg_village}
\end{figure}
% =============================================================================
\section{Discussion}
\label{sec:discussion}
% =============================================================================

% TODO: Analysis of findings, limitations, and future directions.

% =============================================================================
\section{Conclusion}
\label{sec:conclusion}
% =============================================================================

% TODO

\bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}

\appendix
\section{Appendix}
% TODO: Additional results, implementation details, etc.

\end{document}